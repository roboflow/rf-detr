# install required libraries in Google Colab for onnx to tflite conversion
!sudo apt-get -y update
!sudo apt-get -y install python3-pip
!sudo apt-get -y install python-is-python3
!wget https://github.com/PINTO0309/onnx2tf/releases/download/1.16.31/flatc.tar.gz \
  && tar -zxvf flatc.tar.gz \
  && sudo chmod +x flatc \
  && sudo mv flatc /usr/bin/
!pip install -U pip \
  && pip install tensorflow==2.19.0 \
  && pip install ai_edge_litert==1.2.0 \
  && pip install -U onnx==1.17.0 \
  && python -m pip install onnx_graphsurgeon \
        --index-url https://pypi.ngc.nvidia.com \
  && pip install -U onnxruntime==1.18.1 \
  && pip install -U onnxsim==0.4.33 \
  && pip install -U simple_onnx_processing_tools \
  && pip install -U onnx2tf \
  && pip install -U protobuf==3.20.3 \
  && pip install -U h5py==3.11.0 \
  && pip install -U psutil==5.9.5 \
  && pip install -U ml_dtypes==0.5.1 \
  && pip install -U tf-keras==2.19.0 \
  && pip install flatbuffers>=23.5.26

# After the successful installation of the above libraries, run the below command
!onnx2tf -i /content/output/inference_model.onnx

# The above command will convert your onnx model to the tflite format under the saved_model directory

# inference code using FP16 tflite model
# inference using tflite model
import numpy as np
import tensorflow as tf
import cv2
from google.colab.patches import cv2_imshow

# Paths to your TFlite model and input image
MODEL_PATH = ""  # Replace with your fp16 tflite model path
IMAGE_PATH = ""      # Replace with your image path
OUTPUT_PATH = "output_image_tflite.jpg"    # Replace with desired output path

# Define a color palette for different classes (BGR format for OpenCV)
COLORS = [
    (0, 255, 0),    # Green
    (0, 0, 255),    # Red
    (255, 0, 0),    # Blue
    (255, 255, 0),  # Yellow
    (0, 255, 255),  # Cyan
    (255, 0, 255),  # Magenta
    (128, 128, 128),# Gray
    (255, 128, 0),  # Orange
    (128, 0, 128),  # Purple
    (0, 128, 255)   # Light Blue
]

# Define class names (replace with your actual class names)
CLASS_NAMES =[]

# Load the TFlite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)
interpreter.allocate_tensors()

# Get input and output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Load and preprocess the input image
image = cv2.imread(IMAGE_PATH)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
input_shape = input_details[0]['shape']
height, width = image.shape[:2]
new_height, new_width = input_shape[1], input_shape[2]

# Resize image to match model input size
image_resized = cv2.resize(image_rgb, (new_width, new_height))
image_input = np.expand_dims(image_resized, axis=0).astype(np.float32)
image_input = image_input / 255.0  # Normalize to [0, 1]

# Set the input tensor
interpreter.set_tensor(input_details[0]['index'], image_input)

# Run inference
interpreter.invoke()

# Get the output tensors
outputs = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]

# Parse outputs
boxes = outputs[0][0]  # Shape: (300, 4)
scores_and_classes = outputs[1][0]  # Shape: (300, 6)

# Confidence threshold
confidence_threshold = 0.5
detections = []

# Post-process logits and boxes
for i in range(len(boxes)):
    logits = scores_and_classes[i]
    probabilities = 1 / (1 + np.exp(-logits))  # Sigmoid
    score = np.max(probabilities)
    class_id = np.argmax(probabilities)
    
    if score > confidence_threshold:
        box = boxes[i]
        #print(f"Detection {i}: Raw Box={box}, Score={score}, Class={class_id}")
        
        # Assume [x_center, y_center, width, height]
        x_center_norm = box[0]
        y_center_norm = box[1]
        width_norm = box[2]
        height_norm = box[3]
        
        # Scale to model input size (560x560)
        x_center_input = x_center_norm * new_width
        y_center_input = y_center_norm * new_height
        width_input = width_norm * new_width
        height_input = height_norm * new_height
        
        # Convert to corner coordinates
        x_min_input = x_center_input - (width_input / 2)
        y_min_input = y_center_input - (height_input / 2)
        x_max_input = x_center_input + (width_input / 2)
        y_max_input = y_center_input + (height_input / 2)
        
        # Map to original image size (640x640)
        x_min = int(x_min_input * width / new_width)
        y_min = int(y_min_input * height / new_height)
        x_max = int(x_max_input * width / new_width)
        y_max = int(y_max_input * height / new_height)
        
        # Clamp coordinates
        x_min = max(0, min(x_min, width - 1))
        y_min = max(0, min(y_min, height - 1))
        x_max = max(0, min(x_max, width - 1))
        y_max = max(0, min(y_max, height - 1))
        
        detections.append({
            'box': [x_min, y_min, x_max, y_max],
            'score': score,
            'class': class_id
        })


# Draw bounding boxes and labels with class-specific colors and names
for detection in detections:
    x_min, y_min, x_max, y_max = detection['box']
    score = detection['score']
    class_id = detection['class']
    
    # Select color based on class ID
    color = COLORS[class_id % len(COLORS)]
    
    # Use class name instead of class ID
    class_name = CLASS_NAMES[class_id] if class_id < len(CLASS_NAMES) else f"Class {class_id}"
    label = f"{class_name}: {score:.2f}"
    
    # Draw rectangle and text
    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)
    cv2.putText(image, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

# Save the output image with predictions
cv2.imwrite(OUTPUT_PATH, image)
print(f"Output image saved to {OUTPUT_PATH}")

# Optionally, display the image
cv2_imshow(image)

