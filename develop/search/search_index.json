{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RF-DETR: Real-Time SOTA Detection and Segmentation Model","text":"<p>RF-DETR is a real-time transformer architecture for object detection and instance segmentation developed by Roboflow. Built on a DINOv2 vision transformer backbone, RF-DETR delivers state-of-the-art accuracy and latency trade-offs on Microsoft COCO and RF100-VL.</p> <p>RF-DETR uses a DINOv2 vision transformer backbone and supports both detection and instance segmentation in a single, consistent API. All core models and code are released under the Apache 2.0 license.</p>"},{"location":"#install","title":"Install","text":"<p>You can install and use <code>rfdetr</code> in a Python&gt;=3.10 environment. For detailed installation instructions, including installing from source, and setting up a local development environment, check out our install page.</p> <p>Installation</p> <p> </p> pipuv <pre><code>pip install rfdetr\n</code></pre> <pre><code>uv pip install rfdetr\n</code></pre> <p>For uv projects:</p> <pre><code>uv add rfdetr\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<ul> <li> <p>Run Detection Models</p> <p>Load and run pre-trained RF-DETR detection models.</p> <p> Tutorial</p> </li> <li> <p>Run Segmentation Models</p> <p>Load and run pre-trained RF-DETR-Seg segmentation models.</p> <p> Tutorial</p> </li> <li> <p>Train Models</p> <p>Learn how to fine-tune RF-DETR models for detection and segmentation.</p> <p> Tutorial</p> </li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li> <p>Train RF-DETR on a Custom Dataset. Video</p> <p></p> <p>End to end walkthrough of training RF-DETR on a custom dataset.</p> <p> Watch the video</p> </li> <li> <p>Deploy RF-DETR to NVIDIA Jetson. Article</p> <p></p> <p>Instructions for deploying RF-DETR on NVIDIA Jetson with Roboflow Inference.</p> <p> Read the tutorial</p> </li> <li> <p>Train and Deploy RF-DETR with Roboflow</p> <p></p> <p>Cloud training and hardware deployment workflow using Roboflow.</p> <p> Read the tutorial</p> </li> </ul>"},{"location":"#benchmarks","title":"Benchmarks","text":"<p>RF-DETR achieves the best accuracy\u2013latency trade-off among real-time object detection and instance segmentation models \u2014 both on COCO and on the more demanding RF100-VL benchmark (domain adaptability). For detailed benchmark tables and methodology, check out our benchmarks page.</p>"},{"location":"#detection","title":"Detection","text":"Architecture COCO AP<sub>50</sub> COCO AP<sub>50:95</sub> RF100VL AP<sub>50</sub> RF100VL AP<sub>50:95</sub> Latency (ms) Params (M) Resolution RF-DETR-N 67.6 48.4 85.0 57.7 2.3 30.5 384\u00d7384 RF-DETR-S 72.1 53.0 86.7 60.2 3.5 32.1 512\u00d7512 RF-DETR-M 73.6 54.7 87.4 61.2 4.4 33.7 576\u00d7576 RF-DETR-L 75.1 56.5 88.2 62.2 6.8 33.9 704\u00d7704 RF-DETR-XL 77.4 58.6 88.5 62.9 11.5 126.4 700\u00d7700 RF-DETR-2XL 78.5 60.1 89.0 63.2 17.2 126.9 880\u00d7880"},{"location":"#segmentation","title":"Segmentation","text":"Architecture COCO AP<sub>50</sub> COCO AP<sub>50:95</sub> Latency (ms) Params (M) Resolution RF-DETR-Seg-N 63.0 40.3 3.4 33.6 312\u00d7312 RF-DETR-Seg-S 66.2 43.1 4.4 33.7 384\u00d7384 RF-DETR-Seg-M 68.4 45.3 5.9 35.7 432\u00d7432 RF-DETR-Seg-L 70.5 47.1 8.8 36.2 504\u00d7504 RF-DETR-Seg-XL 72.2 48.8 13.5 38.1 624\u00d7624 RF-DETR-Seg-2XL 73.1 49.9 21.8 38.6 768\u00d7768"},{"location":"learn/benchmarks/","title":"Benchmarks","text":"<p>This page reports RF-DETR benchmark results for object detection and instance segmentation on Microsoft COCO and RF100-VL. All benchmark numbers and plots match the latest released checkpoints and tables shown below. Latency values are measured on an NVIDIA T4 with TensorRT in FP16 at batch size 1. For full methodology details and architectural context, see the RF-DETR paper.</p>"},{"location":"learn/benchmarks/#methodology","title":"Methodology","text":"<p>Accuracy is reported using standard COCO metrics computed with pycocotools. For object detection, we report COCO AP50 and COCO AP50:95, and the same metrics are also reported for RF100-VL. COCO results are evaluated on the validation split, following common practice in detector benchmarking. RF100-VL results are averaged across all 100 datasets to reflect performance under diverse real-world data distributions.</p> <p>Latency is measured as single-image inference latency rather than sustained throughput. All latency numbers are obtained on an NVIDIA T4 GPU using TensorRT 10.4 and CUDA 12.4 with FP16 inference and batch size 1. To reduce variance caused by GPU power throttling and thermal effects, a 200 ms buffer is inserted between consecutive forward passes. This procedure improves reproducibility of latency measurements but is not intended to measure maximum throughput.</p> <p>Accuracy and latency are always measured using the same model artifact and the same numerical precision. This avoids reporting FP32 accuracy together with FP16 latency, which can lead to misleading comparisons because naive FP16 conversion can significantly degrade accuracy for some models.</p>"},{"location":"learn/benchmarks/#detection","title":"Detection","text":"Architecture COCO AP<sub>50</sub> COCO AP<sub>50:95</sub> RF100VL AP<sub>50</sub> RF100VL AP<sub>50:95</sub> Latency (ms) Params (M) Resolution RF-DETR-N 67.6 48.4 85.0 57.7 2.3 30.5 384x384 RF-DETR-S 72.1 53.0 86.7 60.2 3.5 32.1 512x512 RF-DETR-M 73.6 54.7 87.4 61.2 4.4 33.7 576x576 RF-DETR-L 75.1 56.5 88.2 62.2 6.8 33.9 704x704 RF-DETR-XL 77.4 58.6 88.5 62.9 11.5 126.4 700x700 RF-DETR-2XL 78.5 60.1 89.0 63.2 17.2 126.9 880x880 YOLO11-N 52.0 37.4 81.4 55.3 2.5 2.6 640x640 YOLO11-S 59.7 44.4 82.3 56.2 3.2 9.4 640x640 YOLO11-M 64.1 48.6 82.5 56.5 5.1 20.1 640x640 YOLO11-L 64.9 49.9 82.2 56.5 6.5 25.3 640x640 YOLO11-X 66.1 50.9 81.7 56.2 10.5 56.9 640x640 YOLO26-N 55.8 40.3 76.7 52.0 1.7 2.6 640x640 YOLO26-S 64.3 47.7 82.7 57.0 2.6 9.4 640x640 YOLO26-M 69.7 52.5 84.4 58.7 4.4 20.1 640x640 YOLO26-L 71.1 54.1 85.0 59.3 5.7 25.3 640x640 YOLO26-X 74.0 56.9 85.6 60.0 9.6 56.9 640x640 LW-DETR-T 60.7 42.9 84.7 57.1 1.9 12.1 640x640 LW-DETR-S 66.8 48.0 85.0 57.4 2.6 14.6 640x640 LW-DETR-M 72.0 52.6 86.8 59.8 4.4 28.2 640x640 LW-DETR-L 74.6 56.1 87.4 61.5 6.9 46.8 640x640 LW-DETR-X 76.9 58.3 87.9 62.1 13.0 118.0 640x640 D-FINE-N 60.2 42.7 84.4 58.2 2.1 3.8 640x640 D-FINE-S 67.6 50.6 85.3 60.3 3.5 10.2 640x640 D-FINE-M 72.6 55.0 85.5 60.6 5.4 19.2 640x640 D-FINE-L 74.9 57.2 86.4 61.6 7.5 31.0 640x640 D-FINE-X 76.8 59.3 86.9 62.2 11.5 62.0 640x640"},{"location":"learn/benchmarks/#segmentation","title":"Segmentation","text":"Architecture COCO AP<sub>50</sub> COCO AP<sub>50:95</sub> Latency (ms) Params (M) Resolution RF-DETR-Seg-N 63.0 40.3 3.4 33.6 312x312 RF-DETR-Seg-S 66.2 43.1 4.4 33.7 384x384 RF-DETR-Seg-M 68.4 45.3 5.9 35.7 432x432 RF-DETR-Seg-L 70.5 47.1 8.8 36.2 504x504 RF-DETR-Seg-XL 72.2 48.8 13.5 38.1 624x624 RF-DETR-Seg-2XL 73.1 49.9 21.8 38.6 768x768 YOLOv8-N-Seg 45.6 28.3 3.5 3.4 640x640 YOLOv8-S-Seg 53.8 34.0 4.2 11.8 640x640 YOLOv8-M-Seg 58.2 37.3 7.0 27.3 640x640 YOLOv8-L-Seg 60.5 39.0 9.7 46.0 640x640 YOLOv8-XL-Seg 61.3 39.5 14.0 71.8 640x640 YOLOv11-N-Seg 47.8 30.0 3.6 2.9 640x640 YOLOv11-S-Seg 55.4 35.0 4.6 10.1 640x640 YOLOv11-M-Seg 60.0 38.5 6.9 22.4 640x640 YOLOv11-L-Seg 61.5 39.5 8.3 27.6 640x640 YOLOv11-XL-Seg 62.4 40.1 13.7 62.1 640x640 YOLO26-N-Seg 54.3 34.7 2.31 2.7 640x640 YOLO26-S-Seg 62.4 40.2 3.47 10.4 640x640 YOLO26-M-Seg 67.8 44.0 6.32 23.6 640x640 YOLO26-L-Seg 69.8 45.5 7.58 28.0 640x640 YOLO26-X-Seg 71.6 46.8 12.92 62.8 640x640"},{"location":"learn/deploy/","title":"Deploy a Trained RF-DETR Model","text":"<p>You can deploy a fine-tuned RF-DETR model to Roboflow.</p> <p>Deploying to Roboflow allows you to create multi-step computer vision applications that run both in the cloud and your own hardware.</p> <p>To deploy your model to Roboflow, run:</p> Object DetectionImage Segmentation <pre><code>from rfdetr import RFDETRNano\n\nx = RFDETRNano(pretrain_weights=\"&lt;path/to/pretrain/weights/dir&gt;\")\nx.deploy_to_roboflow(\n  workspace=\"&lt;your-workspace&gt;\",\n  project_id=\"&lt;your-project-id&gt;\",\n  version=1,\n  api_key=\"&lt;YOUR_API_KEY&gt;\"\n)\n</code></pre> <pre><code>from rfdetr import RFDETRSegMedium\n\nx = RFDETRSegMedium(pretrain_weights=\"&lt;path/to/pretrain/weights/dir&gt;\")\nx.deploy_to_roboflow(\n  workspace=\"&lt;your-workspace&gt;\",\n  project_id=\"&lt;your-project-id&gt;\",\n  version=1,\n  api_key=\"&lt;YOUR_API_KEY&gt;\"\n)\n</code></pre> <p>Above, set your Roboflow Workspace ID, the ID of the project to which you want to upload your model, and your Roboflow API key.</p> <ul> <li>Learn how to find your Workspace and Project ID.</li> <li>Learn how to find your API key.</li> </ul> <p>You can then run your model with Roboflow Inference:</p> Object DetectionImage Segmentation <pre><code>import supervision as sv\nfrom inference import get_model\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\nurl = \"https://media.roboflow.com/dog.jpeg\"\nimage = Image.open(BytesIO(requests.get(url).content))\n\nmodel = get_model(\"rfdetr-large\")  # replace with your Roboflow model ID\n\npredictions = model.infer(image, confidence=0.5)[0]\n\ndetections = sv.Detections.from_inference(predictions)\n\nlabels = [prediction.class_name for prediction in predictions.predictions]\n\nannotated_image = image.copy()\nannotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n\nsv.plot_image(annotated_image)\n</code></pre> <pre><code>import supervision as sv\nfrom inference import get_model\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\nurl = \"https://media.roboflow.com/dog.jpeg\"\nimage = Image.open(BytesIO(requests.get(url).content))\n\nmodel = get_model(\"rfdetr-seg-small\")  # replace with your Roboflow model ID\n\npredictions = model.infer(image, confidence=0.5)[0]\n\ndetections = sv.Detections.from_inference(predictions)\n\nlabels = [prediction.class_name for prediction in predictions.predictions]\n\nannotated_image = image.copy()\nannotated_image = sv.MaskAnnotator().annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Above, replace <code>rfdetr-large</code> with the your Roboflow model ID. You can find this ID from the \"Models\" list in your Roboflow dashboard:</p> <p></p> <p>When you first run this model, your model weights will be cached for local use with Inference.</p> <p>You will then see the results from your fine-tuned model.</p>"},{"location":"learn/export/","title":"Export RF-DETR Model to ONNX","text":"<p>RF-DETR supports exporting models to the ONNX format, which enables interoperability with various inference frameworks and can improve deployment efficiency.</p>"},{"location":"learn/export/#installation","title":"Installation","text":"<p>To export your model, first install the <code>onnxexport</code> extension:</p> <pre><code>pip install \"rfdetr[onnxexport]\"\n</code></pre>"},{"location":"learn/export/#basic-export","title":"Basic Export","text":"<p>Export your trained model to ONNX format:</p> Object DetectionImage Segmentation <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium(pretrain_weights=\"&lt;path/to/checkpoint.pth&gt;\")\n\nmodel.export()\n</code></pre> <pre><code>from rfdetr import RFDETRSegMedium\n\nmodel = RFDETRSegMedium(pretrain_weights=\"&lt;path/to/checkpoint.pth&gt;\")\n\nmodel.export()\n</code></pre> <p>This command saves the ONNX model to the <code>output</code> directory by default.</p>"},{"location":"learn/export/#export-parameters","title":"Export Parameters","text":"<p>The <code>export()</code> method accepts several parameters to customize the export process:</p> Parameter Default Description <code>output_dir</code> <code>\"output\"</code> Directory where the exported ONNX model will be saved. <code>infer_dir</code> <code>None</code> Path to an image file to use for tracing. If not provided, a random dummy image is generated. <code>simplify</code> <code>False</code> Whether to simplify the ONNX model using onnxsim for better compatibility and performance. <code>backbone_only</code> <code>False</code> Export only the backbone feature extractor instead of the full model. <code>opset_version</code> <code>17</code> ONNX opset version to use for export. Higher versions support more operations. <code>verbose</code> <code>True</code> Whether to print verbose export information. <code>force</code> <code>False</code> Force re-export even if simplified model already exists. <code>shape</code> <code>None</code> Input shape as tuple <code>(height, width)</code>. Must be divisible by 14. If not provided, uses the model's default resolution. <code>batch_size</code> <code>1</code> Batch size for the exported model."},{"location":"learn/export/#advanced-export-examples","title":"Advanced Export Examples","text":""},{"location":"learn/export/#export-with-custom-output-directory","title":"Export with Custom Output Directory","text":"<pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium(pretrain_weights=\"&lt;path/to/checkpoint.pth&gt;\")\n\nmodel.export(output_dir=\"exports/my_model\")\n</code></pre>"},{"location":"learn/export/#export-with-simplification","title":"Export with Simplification","text":"<p>Simplifying the ONNX model can improve inference performance and compatibility with various runtimes:</p> <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium(pretrain_weights=\"&lt;path/to/checkpoint.pth&gt;\")\n\nmodel.export(simplify=True)\n</code></pre>"},{"location":"learn/export/#export-with-custom-resolution","title":"Export with Custom Resolution","text":"<p>Export the model with a specific input resolution (must be divisible by 14):</p> <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium(pretrain_weights=\"&lt;path/to/checkpoint.pth&gt;\")\n\nmodel.export(shape=(560, 560))\n</code></pre>"},{"location":"learn/export/#export-backbone-only","title":"Export Backbone Only","text":"<p>Export only the backbone feature extractor for use in custom pipelines:</p> <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium(pretrain_weights=\"&lt;path/to/checkpoint.pth&gt;\")\n\nmodel.export(backbone_only=True)\n</code></pre>"},{"location":"learn/export/#output-files","title":"Output Files","text":"<p>After running the export, you will find the following files in your output directory:</p> <ul> <li><code>inference_model.onnx</code> - The exported ONNX model (or <code>backbone_model.onnx</code> if <code>backbone_only=True</code>)</li> <li><code>inference_model.sim.onnx</code> - The simplified ONNX model (if <code>simplify=True</code>)</li> </ul>"},{"location":"learn/export/#using-the-exported-model","title":"Using the Exported Model","text":"<p>Once exported, you can use the ONNX model with various inference frameworks:</p>"},{"location":"learn/export/#onnx-runtime","title":"ONNX Runtime","text":"<pre><code>import onnxruntime as ort\nimport numpy as np\nfrom PIL import Image\n\n# Load the ONNX model\nsession = ort.InferenceSession(\"output/inference_model.onnx\")\n\n# Prepare input image\nimage = Image.open(\"image.jpg\").convert(\"RGB\")\nimage = image.resize((560, 560))  # Resize to model's input resolution\nimage_array = np.array(image).astype(np.float32) / 255.0\n\n# Normalize\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\nimage_array = (image_array - mean) / std\n\n# Convert to NCHW format\nimage_array = np.transpose(image_array, (2, 0, 1))\nimage_array = np.expand_dims(image_array, axis=0)\n\n# Run inference\noutputs = session.run(None, {\"input\": image_array})\nboxes, labels = outputs\n</code></pre>"},{"location":"learn/export/#next-steps","title":"Next Steps","text":"<p>After exporting your model, you may want to:</p> <ul> <li>Deploy to Roboflow for cloud-based inference and workflow integration</li> <li>Use the ONNX model with TensorRT for optimized GPU inference</li> <li>Integrate with edge deployment frameworks like ONNX Runtime or OpenVINO</li> </ul>"},{"location":"learn/install/","title":"Installation","text":"<p>Welcome to RF-DETR! This guide will help you install and set up RF-DETR for your projects. Whether you're a developer looking to contribute or an end-user ready to start using RF-DETR, we've got you covered.</p>"},{"location":"learn/install/#installation-methods","title":"Installation Methods","text":"<p>RF-DETR supports several installation methods. Choose the option which best fits your workflow.</p> <p>Installation</p> pip (recommended)uvSource Archive <p>The easiest way to install RF-DETR is using <code>pip</code>. This method is recommended for most users.</p> <pre><code>pip install rfdetr\n</code></pre> <p>If you are using <code>uv</code>, you can install RF-DETR using the following command:</p> <pre><code>uv pip install rfdetr\n</code></pre> <p>For <code>uv</code> projects, you can also use:</p> <pre><code>uv add rfdetr\n</code></pre> <p>To install the latest development version of RF-DETR from source without cloning the full repository, run the command below.</p> <pre><code>pip install https://github.com/roboflow/rf-detr/archive/refs/heads/develop.zip\n</code></pre>"},{"location":"learn/install/#dev-environment","title":"Dev Environment","text":"<p>If you plan to contribute to RF-DETR or modify the codebase locally, set up a local development environment using the steps below.</p> <p>Development Setup</p> virtualenvuv <pre><code># Clone the repository and navigate to the root directory\ngit clone --depth 1 -b develop https://github.com/roboflow/rf-detr.git\ncd rf-detr\n\n# Set up a Python virtual environment with a specific Python version (e.g., 3.10)\npython3.10 -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate\n\n# Upgrade pip\npip install --upgrade pip\n\n# Install the package in development mode\npip install -e \".\"\n</code></pre> <pre><code># Clone the repository and navigate to the root directory\ngit clone --depth 1 -b develop https://github.com/roboflow/rf-detr.git\ncd rf-detr\n\n# Pin Python version (optional but recommended)\nuv python pin 3.11\n\n# Sync environment (creates .venv, installs pinned Python, and installs dependencies)\nuv sync\n\n# Install the package in development mode with all extras\nuv pip install -e . --all-extras\n</code></pre>"},{"location":"learn/install/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure you have Python 3.10 or higher installed.</li> <li>For development, it is recommended to use a virtual environment to avoid conflicts with other packages.</li> <li>If you encounter any issues during installation, refer to the troubleshooting section or open an issue on the GitHub repository.</li> </ul>"},{"location":"learn/install/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, here are some common solutions:</p> <ul> <li>Permission Issues: Use <code>pip install --user rfdetr</code> to install the package for your user only.</li> <li>Dependency Conflicts: Use a virtual environment to isolate the installation.</li> <li>Python Version: Ensure you are using Python 3.10 or higher.</li> </ul>"},{"location":"learn/pretrained/","title":"Pretrained","text":"<p>You can run any of the four supported RF-DETR base models -- Nano, Small, Medium, Large -- with Inference, an open source computer vision inference server. The base models are trained on the Microsoft COCO dataset.</p> Run on an ImageRun on a Video FileRun on a Webcam StreamRun on an RTSP Stream <p>To run RF-DETR on an image, use the following code:</p> <pre><code>import os\nimport supervision as sv\nfrom inference import get_model\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\nurl = \"https://media.roboflow.com/dog.jpeg\"\nimage = Image.open(BytesIO(requests.get(url).content))\n\nmodel = get_model(\"rfdetr-large\")\n\npredictions = model.infer(image, confidence=0.5)[0]\n\ndetections = sv.Detections.from_inference(predictions)\n\nlabels = [prediction.class_name for prediction in predictions.predictions]\n\nannotated_image = image.copy()\nannotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Above, replace the image URL with any image you want to use with the model.</p> <p>Here are the results from the code above:</p> <p> RF-DETR Base predictions </p> <p>To run RF-DETR on a video file, use the following code:</p> <pre><code>import supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium()\n\ndef callback(frame, index):\n    detections = model.predict(frame[:, :, ::-1], threshold=0.5)\n\n    labels = [\n        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n        for class_id, confidence\n        in zip(detections.class_id, detections.confidence)\n    ]\n\n    annotated_frame = frame.copy()\n    annotated_frame = sv.BoxAnnotator().annotate(annotated_frame, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n    return annotated_frame\n\nsv.process_video(\n    source_path=&lt;SOURCE_VIDEO_PATH&gt;,\n    target_path=&lt;TARGET_VIDEO_PATH&gt;,\n    callback=callback\n)\n</code></pre> <p>Above, set your <code>SOURCE_VIDEO_PATH</code> and <code>TARGET_VIDEO_PATH</code> to the directories of the video you want to process and where you want to save the results from inference, respectively.</p> <p>To run RF-DETR on a webcam input, use the following code:</p> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium()\n\ncap = cv2.VideoCapture(0)\nwhile True:\n    success, frame = cap.read()\n    if not success:\n        break\n\n    detections = model.predict(frame[:, :, ::-1], threshold=0.5)\n\n    labels = [\n        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n        for class_id, confidence\n        in zip(detections.class_id, detections.confidence)\n    ]\n\n    annotated_frame = frame.copy()\n    annotated_frame = sv.BoxAnnotator().annotate(annotated_frame, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"Webcam\", annotated_frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>To run RF-DETR on an RTSP (Real Time Streaming Protocol) stream, use the following code:</p> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium()\n\ncap = cv2.VideoCapture(&lt;RTSP_STREAM_URL&gt;)\nwhile True:\n    success, frame = cap.read()\n    if not success:\n        break\n\n    detections = model.predict(frame[:, :, ::-1], threshold=0.5)\n\n    labels = [\n        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n        for class_id, confidence\n        in zip(detections.class_id, detections.confidence)\n    ]\n\n    annotated_frame = frame.copy()\n    annotated_frame = sv.BoxAnnotator().annotate(annotated_frame, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"RTSP Stream\", annotated_frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>You can change the RF-DETR model that the code snippet above uses. To do so, update <code>rfdetr-base</code> to any of the following values:</p> <ul> <li><code>rfdetr-nano</code></li> <li><code>rfdetr-small</code></li> <li><code>rfdetr-medium</code></li> <li><code>rfdetr-large</code></li> </ul>"},{"location":"learn/pretrained/#batch-inference","title":"Batch Inference","text":"<p>You can provide <code>.predict()</code> with either a single image or a list of images. When multiple images are supplied, they are processed together in a single forward pass, resulting in a corresponding list of detections.</p> <pre><code>import io\nimport requests\nimport supervision as sv\nfrom PIL import Image\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium()\n\nurls = [\n    \"https://media.roboflow.com/notebooks/examples/dog-2.jpeg\",\n    \"https://media.roboflow.com/notebooks/examples/dog-3.jpeg\"\n]\n\nimages = [Image.open(io.BytesIO(requests.get(url).content)) for url in urls]\n\ndetections_list = model.predict(images, threshold=0.5)\n\nfor image, detections in zip(images, detections_list):\n    labels = [\n        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n        for class_id, confidence\n        in zip(detections.class_id, detections.confidence)\n    ]\n\n    annotated_image = image.copy()\n    annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)\n    annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n\n    sv.plot_image(annotated_image)\n</code></pre>"},{"location":"learn/run/detection/","title":"Run an RF-DETR Object Detection Model","text":"<p>RF-DETR is a real-time transformer architecture for object detection, built on a DINOv2 vision transformer backbone. The base models are trained on the Microsoft COCO dataset and achieve state-of-the-art accuracy and latency trade-offs.</p>"},{"location":"learn/run/detection/#pre-trained-checkpoints","title":"Pre-trained Checkpoints","text":"<p>RF-DETR offers model sizes from Nano to 2XLarge, allowing trade-offs between accuracy, latency, and parameter count. All latency numbers were measured on an NVIDIA T4 using TensorRT, FP16, and batch size 1. Core models (Nano to Large) are licensed under Apache 2.0, while XLarge and 2XLarge use the Platform Model License 1.0 and require a Roboflow account.</p> Size RF-DETR package class Inference package alias COCO AP<sub>50</sub> COCO AP<sub>50:95</sub> Latency (ms) Params (M) Resolution License N <code>RFDETRNano</code> <code>rfdetr-nano</code> 67.6 48.4 2.3 30.5 384x384 Apache 2.0 S <code>RFDETRSmall</code> <code>rfdetr-small</code> 72.1 53.0 3.5 32.1 512x512 Apache 2.0 M <code>RFDETRMedium</code> <code>rfdetr-medium</code> 73.6 54.7 4.4 33.7 576x576 Apache 2.0 L <code>RFDETRLarge</code> <code>rfdetr-large</code> 75.1 56.5 6.8 33.9 704x704 Apache 2.0 XL <code>RFDETRXLarge</code> <code>rfdetr-xlarge</code> 77.4 58.6 11.5 126.4 700x700 PML 1.0 2XL <code>RFDETR2XLarge</code> <code>rfdetr-2xlarge</code> 78.5 60.1 17.2 126.9 880x880 PML 1.0"},{"location":"learn/run/detection/#run-on-an-image","title":"Run on an Image","text":"<p>Perform inference on an image using either the <code>rfdetr</code> package or the <code>inference</code> package. To use a different model size, select the corresponding class or alias from the table above.</p> rfdetrinference <pre><code>import requests\nimport supervision as sv\nfrom PIL import Image\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium()\n\nimage = Image.open(requests.get('https://media.roboflow.com/dog.jpg', stream=True).raw)\ndetections = model.predict(image, threshold=0.5)\n\nlabels = [\n    f\"{COCO_CLASSES[class_id]}\"\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = sv.BoxAnnotator().annotate(image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n</code></pre> <pre><code>import requests\nimport supervision as sv\nfrom PIL import Image\nfrom inference import get_model\n\nmodel = get_model(\"rfdetr-medium\")\n\nimage = Image.open(requests.get('https://media.roboflow.com/dog.jpg', stream=True).raw)\npredictions = model.infer(image, confidence=0.5)[0]\ndetections = sv.Detections.from_inference(predictions)\n\nannotated_image = sv.BoxAnnotator().annotate(image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections)\n</code></pre>"},{"location":"learn/run/detection/#run-on-video-webcam-or-rtsp-stream","title":"Run on video, webcam, or RTSP stream","text":"<p>These examples use OpenCV for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code>, <code>&lt;WEBCAM_INDEX&gt;</code>, and <code>&lt;RTSP_STREAM_URL&gt;</code> with your inputs. <code>&lt;WEBCAM_INDEX&gt;</code> is usually <code>0</code> for the default camera.</p> videowebcamstream <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source: &lt;SOURCE_VIDEO_PATH&gt;\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb, threshold=0.5)\n\n    labels = [\n        COCO_CLASSES[class_id]\n        for class_id in detections.class_id\n    ]\n\n    annotated_frame = sv.BoxAnnotator().annotate(frame_bgr, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"RF-DETR Video\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium()\n\nvideo_capture = cv2.VideoCapture(&lt;WEBCAM_INDEX&gt;)\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open webcam: &lt;WEBCAM_INDEX&gt;\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb, threshold=0.5)\n\n    labels = [\n        COCO_CLASSES[class_id]\n        for class_id in detections.class_id\n    ]\n\n    annotated_frame = sv.BoxAnnotator().annotate(frame_bgr, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"RF-DETR Webcam\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium()\n\nvideo_capture = cv2.VideoCapture(\"&lt;RTSP_STREAM_URL&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open RTSP stream: &lt;RTSP_STREAM_URL&gt;\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb, threshold=0.5)\n\n    labels = [\n        COCO_CLASSES[class_id]\n        for class_id in detections.class_id\n    ]\n\n    annotated_frame = sv.BoxAnnotator().annotate(frame_bgr, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"RF-DETR RTSP\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"learn/run/segmentation/","title":"Run an RF-DETR Instance Segmentation Model","text":"<p>RF-DETR is a real-time transformer architecture for instance segmentation, built on a DINOv2 vision transformer backbone. The base models are trained on the Microsoft COCO dataset and achieve strong accuracy and latency trade-offs.</p>"},{"location":"learn/run/segmentation/#pre-trained-checkpoints","title":"Pre-trained Checkpoints","text":"<p>RF-DETR-Seg offers model sizes from Nano to 2XLarge, allowing trade-offs between accuracy, latency, and parameter count. All latency numbers were measured on an NVIDIA T4 using TensorRT, FP16, and batch size 1.</p> Size RF-DETR package class Inference package alias COCO AP<sub>50</sub> COCO AP<sub>50:95</sub> Latency (ms) Params (M) Resolution N <code>RFDETRSegNano</code> <code>rfdetr-seg-nano</code> 63.0 40.3 3.4 33.6 312x312 S <code>RFDETRSegSmall</code> <code>rfdetr-seg-small</code> 66.2 43.1 4.4 33.7 384x384 M <code>RFDETRSegMedium</code> <code>rfdetr-seg-medium</code> 68.4 45.3 5.9 35.7 432x432 L <code>RFDETRSegLarge</code> <code>rfdetr-seg-large</code> 70.5 47.1 8.8 36.2 504x504 XL <code>RFDETRSegXLarge</code> <code>rfdetr-seg-xlarge</code> 72.2 48.8 13.5 38.1 624x624 2XL <code>RFDETRSeg2XLarge</code> <code>rfdetr-seg-2xlarge</code> 73.1 49.9 21.8 38.6 768x768"},{"location":"learn/run/segmentation/#run-on-an-image","title":"Run on an Image","text":"<p>Perform inference on an image using either the <code>rfdetr</code> package or the <code>inference</code> package. To use a different model size, select the corresponding class or alias from the table above.</p> rfdetrinference <pre><code>import requests\nimport supervision as sv\nfrom PIL import Image\nfrom rfdetr import RFDETRSegMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRSegMedium()\n\nimage = Image.open(requests.get('https://media.roboflow.com/dog.jpg', stream=True).raw)\ndetections = model.predict(image, threshold=0.5)\n\nlabels = [\n    f\"{COCO_CLASSES[class_id]}\"\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = sv.MaskAnnotator().annotate(image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n</code></pre> <pre><code>import requests\nimport supervision as sv\nfrom PIL import Image\nfrom inference import get_model\n\nmodel = get_model(\"rfdetr-seg-medium\")\n\nimage = Image.open(requests.get('https://media.roboflow.com/dog.jpg', stream=True).raw)\npredictions = model.infer(image, confidence=0.5)[0]\ndetections = sv.Detections.from_inference(predictions)\n\nannotated_image = sv.MaskAnnotator().annotate(image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections)\n</code></pre>"},{"location":"learn/run/segmentation/#run-on-video-webcam-or-rtsp-stream","title":"Run on video, webcam, or RTSP stream","text":"<p>These examples use OpenCV for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code>, <code>&lt;WEBCAM_INDEX&gt;</code>, and <code>&lt;RTSP_STREAM_URL&gt;</code> with your inputs. <code>&lt;WEBCAM_INDEX&gt;</code> is usually <code>0</code> for the default camera.</p> videowebcamstream <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRSegMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRSegMedium()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source: &lt;SOURCE_VIDEO_PATH&gt;\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb, threshold=0.5)\n\n    labels = [\n        COCO_CLASSES[class_id]\n        for class_id in detections.class_id\n    ]\n\n    annotated_frame = sv.MaskAnnotator().annotate(frame_bgr, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"RF-DETR-Seg Video\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRSegMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRSegMedium()\n\nvideo_capture = cv2.VideoCapture(&lt;WEBCAM_INDEX&gt;)\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open webcam: &lt;WEBCAM_INDEX&gt;\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb, threshold=0.5)\n\n    labels = [\n        COCO_CLASSES[class_id]\n        for class_id in detections.class_id\n    ]\n\n    annotated_frame = sv.MaskAnnotator().annotate(frame_bgr, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"RF-DETR-Seg Webcam\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRSegMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRSegMedium()\n\nvideo_capture = cv2.VideoCapture(\"&lt;RTSP_STREAM_URL&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open RTSP stream: &lt;RTSP_STREAM_URL&gt;\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb, threshold=0.5)\n\n    labels = [\n        COCO_CLASSES[class_id]\n        for class_id in detections.class_id\n    ]\n\n    annotated_frame = sv.MaskAnnotator().annotate(frame_bgr, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"RF-DETR-Seg RTSP\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"learn/train/","title":"Train an RF-DETR Model","text":"<p>You can train RF-DETR object detection and segmentation models on a custom dataset using the <code>rfdetr</code> Python package, or in the cloud using Roboflow.</p> <p>This guide describes how to train both an object detection and segmentation RF-DETR model.</p>"},{"location":"learn/train/#quick-start","title":"Quick Start","text":"<p>RF-DETR supports training on datasets in both COCO and YOLO formats. The format is automatically detected based on the structure of your dataset directory.</p> Object DetectionImage Segmentation <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium()\n\nmodel.train(\n    dataset_dir=&lt;DATASET_PATH&gt;,\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=&lt;OUTPUT_PATH&gt;\n)\n</code></pre> <pre><code>from rfdetr import RFDETRSegMedium\n\nmodel = RFDETRSegMedium()\n\nmodel.train(\n    dataset_dir=&lt;DATASET_PATH&gt;,\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=&lt;OUTPUT_PATH&gt;\n)\n</code></pre> <p>Different GPUs have different VRAM capacities, so adjust batch_size and grad_accum_steps to maintain a total batch size of 16. For example, on a powerful GPU like the A100, use <code>batch_size=16</code> and <code>grad_accum_steps=1</code>; on smaller GPUs like the T4, use <code>batch_size=4</code> and <code>grad_accum_steps=4</code>. This gradient accumulation strategy helps train effectively even with limited memory.</p> <p>For object detection, the RF-DETR-B checkpoint is used by default. To get started quickly with training an object detection model, please refer to our fine-tuning Google Colab notebook.</p>"},{"location":"learn/train/#dataset-format","title":"Dataset Format","text":"<p>RF-DETR automatically detects whether your dataset is in COCO or YOLO format. Simply pass your dataset directory to the <code>train()</code> method and the appropriate data loader will be used.</p> Format Detection Method Learn More COCO Looks for <code>train/_annotations.coco.json</code> COCO Format Guide YOLO Looks for <code>data.yaml</code> + <code>train/images/</code> YOLO Format Guide <p>Roboflow allows you to create object detection datasets from scratch and export them in either COCO JSON or YOLO format for training. You can also explore Roboflow Universe to find pre-labeled datasets for a range of use cases.</p> <p>\u2192 Learn more about dataset formats</p>"},{"location":"learn/train/#training-configuration","title":"Training Configuration","text":"<p>RF-DETR provides many configuration options to customize your training run. See the complete reference for all available parameters.</p> <p>\u2192 View all training parameters</p>"},{"location":"learn/train/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Resume training from a checkpoint</li> <li>Early stopping to prevent overfitting</li> <li>Multi-GPU training with PyTorch DDP</li> <li>Logging with TensorBoard</li> <li>Logging with Weights and Biases</li> </ul> <p>\u2192 Learn more about advanced training</p>"},{"location":"learn/train/#result-checkpoints","title":"Result Checkpoints","text":"<p>During training, multiple model checkpoints are saved to the output directory:</p> <ul> <li> <p><code>checkpoint.pth</code> \u2013 the most recent checkpoint, saved at the end of the latest epoch.</p> </li> <li> <p><code>checkpoint_&lt;number&gt;.pth</code> \u2013 periodic checkpoints saved every N epochs (default is every 10).</p> </li> <li> <p><code>checkpoint_best_ema.pth</code> \u2013 best checkpoint based on validation score, using the EMA (Exponential Moving Average) weights. EMA weights are a smoothed version of the model's parameters across training steps, often yielding better generalization.</p> </li> <li> <p><code>checkpoint_best_regular.pth</code> \u2013 best checkpoint based on validation score, using the raw (non-EMA) model weights.</p> </li> <li> <p><code>checkpoint_best_total.pth</code> \u2013 final checkpoint selected for inference and benchmarking. It contains only the model weights (no optimizer state or scheduler) and is chosen as the better of the EMA and non-EMA models based on validation performance.</p> </li> </ul> Checkpoint file sizes <p>Checkpoint sizes vary based on what they contain:</p> <ul> <li> <p>Training checkpoints (e.g. <code>checkpoint.pth</code>, <code>checkpoint_&lt;number&gt;.pth</code>) include model weights, optimizer state, scheduler state, and training metadata. Use these to resume training.</p> </li> <li> <p>Evaluation checkpoints (e.g. <code>checkpoint_best_ema.pth</code>, <code>checkpoint_best_regular.pth</code>) store only the model weights \u2014 either EMA or raw \u2014 and are used to track the best-performing models. These may come from different epochs depending on which version achieved the highest validation score.</p> </li> <li> <p>Stripped checkpoint (e.g. <code>checkpoint_best_total.pth</code>) contains only the final model weights and is optimized for inference and deployment.</p> </li> </ul>"},{"location":"learn/train/#load-and-run-fine-tuned-model","title":"Load and Run Fine-Tuned Model","text":"Object DetectionImage Segmentation <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium(pretrain_weights=&lt;CHECKPOINT_PATH&gt;)\n\ndetections = model.predict(&lt;IMAGE_PATH&gt;)\n</code></pre> <pre><code>from rfdetr import RFDETRSegMedium\n\nmodel = RFDETRSegMedium(pretrain_weights=&lt;CHECKPOINT_PATH&gt;)\n\ndetections = model.predict(&lt;IMAGE_PATH&gt;)\n</code></pre>"},{"location":"learn/train/#next-steps","title":"Next Steps","text":"<p>After training your model, you can:</p> <ul> <li>Export your model to ONNX for deployment with various inference frameworks</li> <li>Deploy to Roboflow for cloud-based inference and workflow integration</li> </ul>"},{"location":"learn/train/advanced/","title":"Advanced Training","text":"<p>This page covers advanced training topics including resuming training, early stopping, multi-GPU training, and logging with external services.</p>"},{"location":"learn/train/advanced/#resume-training","title":"Resume Training","text":"<p>You can resume training from a previously saved checkpoint by passing the path to the <code>checkpoint.pth</code> file using the <code>resume</code> argument. This is useful when training is interrupted or you want to continue fine-tuning an already partially trained model.</p> <p>The training loop will automatically load: - Model weights - Optimizer state - Learning rate scheduler state - Training epoch number</p> Object DetectionImage Segmentation <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium()\n\nmodel.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=\"output\",\n    resume=\"output/checkpoint.pth\"\n)\n</code></pre> <pre><code>from rfdetr import RFDETRSegMedium\n\nmodel = RFDETRSegMedium()\n\nmodel.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=\"output\",\n    resume=\"output/checkpoint.pth\"\n)\n</code></pre> <p>Resume vs Pretrain Weights</p> <ul> <li>Use <code>resume=\"checkpoint.pth\"</code> to continue training with optimizer state</li> <li>Use <code>pretrain_weights=\"checkpoint_best_total.pth\"</code> when initializing a model to start fresh training from those weights</li> </ul>"},{"location":"learn/train/advanced/#early-stopping","title":"Early Stopping","text":"<p>Early stopping monitors validation mAP and halts training if improvements remain below a threshold for a set number of epochs. This prevents wasted computation once the model has converged.</p>"},{"location":"learn/train/advanced/#basic-usage","title":"Basic Usage","text":"Object DetectionImage Segmentation <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium()\n\nmodel.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=\"output\",\n    early_stopping=True\n)\n</code></pre> <pre><code>from rfdetr import RFDETRSegMedium\n\nmodel = RFDETRSegMedium()\n\nmodel.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=\"output\",\n    early_stopping=True\n)\n</code></pre>"},{"location":"learn/train/advanced/#configuration-options","title":"Configuration Options","text":"Parameter Default Description <code>early_stopping_patience</code> 10 Number of epochs without improvement before stopping <code>early_stopping_min_delta</code> 0.001 Minimum mAP change to count as improvement <code>early_stopping_use_ema</code> False Use EMA model's mAP for comparisons"},{"location":"learn/train/advanced/#advanced-example","title":"Advanced Example","text":"<pre><code>model.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=200,\n    early_stopping=True,\n    early_stopping_patience=15,      # Wait 15 epochs before stopping\n    early_stopping_min_delta=0.005,  # Require 0.5% mAP improvement\n    early_stopping_use_ema=True      # Track EMA model performance\n)\n</code></pre>"},{"location":"learn/train/advanced/#how-it-works","title":"How It Works","text":"<ol> <li>After each epoch, validation mAP is computed</li> <li>If mAP improves by at least <code>min_delta</code>, the patience counter resets</li> <li>If mAP doesn't improve, the patience counter increments</li> <li>When patience counter reaches <code>patience</code>, training stops</li> <li>The best checkpoint is already saved as <code>checkpoint_best_total.pth</code></li> </ol> <pre><code>Epoch 10: mAP = 0.450 (best: 0.450) - counter: 0\nEpoch 11: mAP = 0.455 (best: 0.455) - counter: 0 (improved)\nEpoch 12: mAP = 0.454 (best: 0.455) - counter: 1 (no improvement)\nEpoch 13: mAP = 0.453 (best: 0.455) - counter: 2\n...\nEpoch 22: mAP = 0.452 (best: 0.455) - counter: 10 \u2192 STOP\n</code></pre>"},{"location":"learn/train/advanced/#multi-gpu-training","title":"Multi-GPU Training","text":"<p>You can fine-tune RF-DETR on multiple GPUs using PyTorch's Distributed Data Parallel (DDP). This splits the workload across GPUs for faster training.</p>"},{"location":"learn/train/advanced/#setup","title":"Setup","text":"<ol> <li> <p>Create a training script (<code>main.py</code>):</p> <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium()\n\nmodel.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=1,\n    lr=1e-4,\n    output_dir=\"output\"\n)\n</code></pre> </li> <li> <p>Run with <code>torch.distributed.launch</code>:</p> <pre><code>python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py\n</code></pre> </li> </ol> <p>Replace <code>8</code> with the number of GPUs you want to use.</p>"},{"location":"learn/train/advanced/#batch-size-with-multiple-gpus","title":"Batch Size with Multiple GPUs","text":"<p>When using multiple GPUs, your effective batch size is multiplied by the number of GPUs:</p> <pre><code>effective_batch_size = batch_size \u00d7 grad_accum_steps \u00d7 num_gpus\n</code></pre> <p>Example configurations for effective batch size of 16:</p> GPUs <code>batch_size</code> <code>grad_accum_steps</code> Effective 1 4 4 16 2 4 2 16 4 4 1 16 8 2 1 16 <p>Adjust for GPU count</p> <p>When switching between single and multi-GPU training, remember to adjust <code>batch_size</code> and <code>grad_accum_steps</code> to maintain the same effective batch size.</p>"},{"location":"learn/train/advanced/#multi-node-training","title":"Multi-Node Training","text":"<p>For training across multiple machines, use <code>torchrun</code>:</p> <pre><code>torchrun \\\n    --nproc_per_node=8 \\\n    --nnodes=2 \\\n    --node_rank=0 \\\n    --master_addr=\"192.168.1.1\" \\\n    --master_port=1234 \\\n    main.py\n</code></pre> <p>Run this command on each node, changing <code>--node_rank</code> accordingly.</p>"},{"location":"learn/train/advanced/#logging-with-tensorboard","title":"Logging with TensorBoard","text":"<p>TensorBoard is a powerful toolkit for visualizing and tracking training metrics.</p>"},{"location":"learn/train/advanced/#setup_1","title":"Setup","text":"<ol> <li> <p>Install the required packages:</p> <pre><code>pip install \"rfdetr[metrics]\"\n</code></pre> </li> <li> <p>Enable TensorBoard logging in your training:</p> <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium()\n\nmodel.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=\"output\",\n    tensorboard=True\n)\n</code></pre> </li> </ol>"},{"location":"learn/train/advanced/#viewing-logs","title":"Viewing Logs","text":"<p>Local environment:</p> <pre><code>tensorboard --logdir output\n</code></pre> <p>Then open <code>http://localhost:6006/</code> in your browser.</p> <p>Google Colab:</p> <pre><code>%load_ext tensorboard\n%tensorboard --logdir output\n</code></pre>"},{"location":"learn/train/advanced/#logged-metrics","title":"Logged Metrics","text":"<p>TensorBoard will track:</p> <ul> <li>Training loss (total and per-component)</li> <li>Validation mAP</li> <li>Learning rate schedule</li> <li>EMA model metrics (when enabled)</li> </ul>"},{"location":"learn/train/advanced/#logging-with-weights-and-biases","title":"Logging with Weights and Biases","text":"<p>Weights and Biases (W&amp;B) is a cloud-based platform for experiment tracking and visualization.</p>"},{"location":"learn/train/advanced/#setup_2","title":"Setup","text":"<ol> <li> <p>Install the required packages:</p> <pre><code>pip install \"rfdetr[metrics]\"\n</code></pre> </li> <li> <p>Log in to W&amp;B:</p> <pre><code>wandb login\n</code></pre> <p>You can retrieve your API key at wandb.ai/authorize.</p> </li> <li> <p>Enable W&amp;B logging in your training:</p> <pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium()\n\nmodel.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=\"output\",\n    wandb=True,\n    project=\"my-detection-project\",\n    run=\"experiment-001\"\n)\n</code></pre> </li> </ol>"},{"location":"learn/train/advanced/#wb-organization","title":"W&amp;B Organization","text":"Parameter Description <code>project</code> Groups related experiments together <code>run</code> Identifies individual training sessions <p>If you don't specify a run name, W&amp;B assigns a random one automatically.</p>"},{"location":"learn/train/advanced/#viewing-results","title":"Viewing Results","text":"<p>Access your runs at wandb.ai. W&amp;B provides:</p> <ul> <li>Real-time metric visualization</li> <li>Experiment comparison</li> <li>Hyperparameter tracking</li> <li>System metrics (GPU usage, memory)</li> <li>Training config logging</li> </ul>"},{"location":"learn/train/advanced/#using-both-tensorboard-and-wb","title":"Using Both TensorBoard and W&amp;B","text":"<p>You can enable both logging systems simultaneously:</p> <pre><code>model.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    tensorboard=True,\n    wandb=True,\n    project=\"my-project\",\n    run=\"experiment-001\"\n)\n</code></pre>"},{"location":"learn/train/advanced/#memory-optimization","title":"Memory Optimization","text":""},{"location":"learn/train/advanced/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>For large models or high resolutions, enable gradient checkpointing to trade compute for memory:</p> <pre><code>model.train(\n    dataset_dir=\"path/to/dataset\",\n    gradient_checkpointing=True,\n    batch_size=2,  # May be able to increase with checkpointing\n)\n</code></pre> <p>This re-computes activations during the backward pass instead of storing them, reducing memory usage by ~30-40% at the cost of ~20% slower training.</p>"},{"location":"learn/train/advanced/#memory-efficient-configurations","title":"Memory-Efficient Configurations","text":"Memory Level Configuration Very Low (8GB) <code>batch_size=1</code>, <code>grad_accum_steps=16</code>, <code>gradient_checkpointing=True</code>, <code>resolution=560</code> Low (12GB) <code>batch_size=2</code>, <code>grad_accum_steps=8</code>, <code>gradient_checkpointing=True</code> Medium (16GB) <code>batch_size=4</code>, <code>grad_accum_steps=4</code> High (24GB) <code>batch_size=8</code>, <code>grad_accum_steps=2</code> Very High (40GB+) <code>batch_size=16</code>, <code>grad_accum_steps=1</code>, <code>resolution=784</code>"},{"location":"learn/train/advanced/#training-tips","title":"Training Tips","text":""},{"location":"learn/train/advanced/#learning-rate-tuning","title":"Learning Rate Tuning","text":"<ul> <li>Fine-tuning from COCO weights (default): Use default learning rates (<code>lr=1e-4</code>, <code>lr_encoder=1.5e-4</code>)</li> <li>Small dataset (&lt;1000 images): Consider lower <code>lr</code> (e.g., <code>5e-5</code>) to prevent overfitting</li> <li>Large dataset (&gt;10000 images): May benefit from higher <code>lr</code> (e.g., <code>2e-4</code>)</li> </ul>"},{"location":"learn/train/advanced/#epoch-count","title":"Epoch Count","text":"Dataset Size Recommended Epochs &lt; 500 images 100-200 500-2000 images 50-100 2000-10000 images 30-50 &gt; 10000 images 20-30 <p>Use early stopping to automatically determine the optimal stopping point.</p>"},{"location":"learn/train/advanced/#data-augmentation","title":"Data Augmentation","text":"<p>RF-DETR applies built-in augmentations during training:</p> <ul> <li>Random resizing</li> <li>Random cropping</li> <li>Color jittering</li> <li>Horizontal flipping</li> </ul> <p>These are automatically configured and don't require manual setup.</p>"},{"location":"learn/train/advanced/#troubleshooting","title":"Troubleshooting","text":""},{"location":"learn/train/advanced/#out-of-memory-oom","title":"Out of Memory (OOM)","text":"<p>If you encounter CUDA out of memory errors:</p> <ol> <li>Reduce <code>batch_size</code></li> <li>Enable <code>gradient_checkpointing=True</code></li> <li>Reduce <code>resolution</code></li> <li>Increase <code>grad_accum_steps</code> to maintain effective batch size</li> </ol>"},{"location":"learn/train/advanced/#training-too-slow","title":"Training Too Slow","text":"<ol> <li>Increase <code>batch_size</code> (if memory allows)</li> <li>Use multiple GPUs with DDP</li> <li>Ensure you're using GPU (check <code>device=\"cuda\"</code>)</li> <li>Consider using a smaller model (e.g., <code>RFDETRSmall</code> instead of <code>RFDETRLarge</code>)</li> </ol>"},{"location":"learn/train/advanced/#loss-not-decreasing","title":"Loss Not Decreasing","text":"<ol> <li>Check that your dataset is correctly formatted</li> <li>Verify annotations are correct (bounding boxes in correct format)</li> <li>Try reducing the learning rate</li> <li>Check for class imbalance in your dataset</li> </ol>"},{"location":"learn/train/dataset-formats/","title":"Dataset Formats","text":"<p>RF-DETR supports training on datasets in two popular formats: COCO and YOLO. The format is automatically detected based on your dataset's directory structure\u2014simply pass your dataset directory to the <code>train()</code> method.</p>"},{"location":"learn/train/dataset-formats/#automatic-format-detection","title":"Automatic Format Detection","text":"<p>When you call <code>model.train(dataset_dir=&lt;path&gt;)</code>, RF-DETR checks the following:</p> <ol> <li>COCO format: Looks for <code>train/_annotations.coco.json</code></li> <li>YOLO format: Looks for <code>data.yaml</code> (or <code>data.yml</code>) and <code>train/images/</code> directory</li> </ol> <p>If neither format is detected, an error is raised with instructions on what's expected.</p> <p>Roboflow Export</p> <p>Roboflow can export datasets in both COCO and YOLO formats. When downloading from Roboflow, select the appropriate format based on your preference.</p>"},{"location":"learn/train/dataset-formats/#coco-format","title":"COCO Format","text":"<p>COCO (Common Objects in Context) format uses JSON files to store annotations in a structured format with images, categories, and annotations.</p>"},{"location":"learn/train/dataset-formats/#directory-structure","title":"Directory Structure","text":"<pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 _annotations.coco.json\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ... (other image files)\n\u251c\u2500\u2500 valid/\n\u2502   \u251c\u2500\u2500 _annotations.coco.json\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ... (other image files)\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 _annotations.coco.json\n    \u251c\u2500\u2500 image1.jpg\n    \u251c\u2500\u2500 image2.jpg\n    \u2514\u2500\u2500 ... (other image files)\n</code></pre>"},{"location":"learn/train/dataset-formats/#annotation-file-structure","title":"Annotation File Structure","text":"<p>Each <code>_annotations.coco.json</code> file contains:</p> <pre><code>{\n  \"info\": {\n    \"description\": \"Dataset description\",\n    \"version\": \"1.0\"\n  },\n  \"licenses\": [],\n  \"images\": [\n    {\n      \"id\": 1,\n      \"file_name\": \"image1.jpg\",\n      \"width\": 640,\n      \"height\": 480\n    }\n  ],\n  \"categories\": [\n    {\n      \"id\": 1,\n      \"name\": \"cat\",\n      \"supercategory\": \"animal\"\n    },\n    {\n      \"id\": 2,\n      \"name\": \"dog\",\n      \"supercategory\": \"animal\"\n    }\n  ],\n  \"annotations\": [\n    {\n      \"id\": 1,\n      \"image_id\": 1,\n      \"category_id\": 1,\n      \"bbox\": [100, 150, 200, 180],\n      \"area\": 36000,\n      \"iscrowd\": 0\n    }\n  ]\n}\n</code></pre>"},{"location":"learn/train/dataset-formats/#key-fields","title":"Key Fields","text":"Field Description <code>images</code> List of image metadata including <code>id</code>, <code>file_name</code>, <code>width</code>, <code>height</code> <code>categories</code> List of object categories with <code>id</code> and <code>name</code> <code>annotations</code> List of object annotations linking images to categories <code>bbox</code> Bounding box in <code>[x, y, width, height]</code> format (top-left corner) <code>area</code> Area of the bounding box <code>iscrowd</code> 0 for individual objects, 1 for crowd regions"},{"location":"learn/train/dataset-formats/#segmentation-annotations","title":"Segmentation Annotations","text":"<p>For training segmentation models, your COCO annotations must include a <code>segmentation</code> key with polygon coordinates:</p> <pre><code>{\n  \"id\": 1,\n  \"image_id\": 1,\n  \"category_id\": 1,\n  \"bbox\": [100, 150, 200, 180],\n  \"area\": 36000,\n  \"iscrowd\": 0,\n  \"segmentation\": [[100, 150, 150, 150, 200, 200, 150, 250, 100, 200]]\n}\n</code></pre> <p>The <code>segmentation</code> field contains a list of polygons, where each polygon is a flat list of coordinates: <code>[x1, y1, x2, y2, x3, y3, ...]</code>.</p>"},{"location":"learn/train/dataset-formats/#yolo-format","title":"YOLO Format","text":"<p>YOLO format uses separate text files for each image's annotations and a <code>data.yaml</code> configuration file that defines class names.</p>"},{"location":"learn/train/dataset-formats/#directory-structure_1","title":"Directory Structure","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 valid/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image1.jpg\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image1.txt\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"learn/train/dataset-formats/#datayaml-configuration","title":"data.yaml Configuration","text":"<p>The <code>data.yaml</code> file at the root of your dataset directory defines the class names:</p> <pre><code>names:\n  - cat\n  - dog\n  - bird\n\nnc: 3\n\ntrain: train/images\nval: valid/images\ntest: test/images\n</code></pre> Field Description <code>names</code> List of class names (0-indexed) <code>nc</code> Number of classes <code>train</code>, <code>val</code>, <code>test</code> Paths to image directories (relative to data.yaml) <p>Alternative format</p> <p>Some YOLO datasets use a dictionary format for names: <pre><code>names:\n  0: cat\n  1: dog\n  2: bird\n</code></pre> Both formats are supported.</p>"},{"location":"learn/train/dataset-formats/#label-file-format","title":"Label File Format","text":"<p>Each image has a corresponding <code>.txt</code> file in the <code>labels/</code> directory with the same base name. Each line in the label file represents one object:</p> <pre><code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\n</code></pre> <p>Example (<code>image1.txt</code>): <pre><code>0 0.5 0.4 0.3 0.2\n1 0.2 0.6 0.15 0.25\n</code></pre></p>"},{"location":"learn/train/dataset-formats/#coordinate-format","title":"Coordinate Format","text":"Field Range Description <code>class_id</code> 0, 1, 2, ... Zero-indexed class ID from <code>names</code> in data.yaml <code>x_center</code> 0.0 - 1.0 Normalized x-coordinate of bounding box center <code>y_center</code> 0.0 - 1.0 Normalized y-coordinate of bounding box center <code>width</code> 0.0 - 1.0 Normalized width of bounding box <code>height</code> 0.0 - 1.0 Normalized height of bounding box <p>All coordinates are normalized relative to image dimensions. For example, if an image is 640\u00d7480 pixels and the bounding box center is at (320, 240):</p> <ul> <li><code>x_center</code> = 320 / 640 = 0.5</li> <li><code>y_center</code> = 240 / 480 = 0.5</li> </ul>"},{"location":"learn/train/dataset-formats/#segmentation-labels-yolo-seg","title":"Segmentation Labels (YOLO-Seg)","text":"<p>For segmentation, YOLO format extends the label format with polygon coordinates:</p> <pre><code>&lt;class_id&gt; &lt;x1&gt; &lt;y1&gt; &lt;x2&gt; &lt;y2&gt; &lt;x3&gt; &lt;y3&gt; ...\n</code></pre> <p>Example (<code>image1.txt</code> with segmentation): <pre><code>0 0.1 0.2 0.3 0.2 0.4 0.5 0.2 0.6 0.1 0.4\n</code></pre></p> <p>The coordinates after the class ID represent the polygon vertices in normalized format.</p>"},{"location":"learn/train/dataset-formats/#converting-between-formats","title":"Converting Between Formats","text":""},{"location":"learn/train/dataset-formats/#yolo-to-coco","title":"YOLO to COCO","text":"<p>You can use the supervision library to convert datasets:</p> <pre><code>import supervision as sv\n\n# Load YOLO dataset\ndataset = sv.DetectionDataset.from_yolo(\n    images_directory_path=\"path/to/images\",\n    annotations_directory_path=\"path/to/labels\",\n    data_yaml_path=\"path/to/data.yaml\"\n)\n\n# Save as COCO\ndataset.as_coco(\n    images_directory_path=\"output/images\",\n    annotations_path=\"output/annotations.json\"\n)\n</code></pre>"},{"location":"learn/train/dataset-formats/#coco-to-yolo","title":"COCO to YOLO","text":"<pre><code>import supervision as sv\n\n# Load COCO dataset\ndataset = sv.DetectionDataset.from_coco(\n    images_directory_path=\"path/to/images\",\n    annotations_path=\"path/to/annotations.json\"\n)\n\n# Save as YOLO\ndataset.as_yolo(\n    images_directory_path=\"output/images\",\n    annotations_directory_path=\"output/labels\",\n    data_yaml_path=\"output/data.yaml\"\n)\n</code></pre>"},{"location":"learn/train/dataset-formats/#using-roboflow","title":"Using Roboflow","text":"<p>Roboflow provides a web interface to:</p> <ol> <li>Upload datasets in any format</li> <li>Annotate new images or edit existing annotations</li> <li>Export in COCO, YOLO, or other formats</li> </ol> <p>This is often the easiest way to convert between formats while also having the option to augment your data.</p>"},{"location":"learn/train/dataset-formats/#which-format-should-i-use","title":"Which Format Should I Use?","text":"<p>Both formats work equally well with RF-DETR. Choose based on your workflow:</p> Consideration COCO YOLO Annotation storage Single JSON file per split One text file per image Human readability JSON structure, verbose Simple text, compact Other framework compatibility DETR family, MMDetection Ultralytics YOLO Segmentation support Full polygon support Full polygon support Editing annotations Requires JSON parsing Simple text editing <p>Recommendation</p> <p>If you're exporting from Roboflow or already have a dataset in one format, simply use that format. RF-DETR handles both identically.</p>"},{"location":"learn/train/dataset-formats/#troubleshooting","title":"Troubleshooting","text":""},{"location":"learn/train/dataset-formats/#format-detection-fails","title":"Format Detection Fails","text":"<p>If you see an error like: <pre><code>Could not detect dataset format in /path/to/dataset\n</code></pre></p> <p>Check that:</p> <p>For COCO format: - <code>train/_annotations.coco.json</code> exists - The JSON file is valid</p> <p>For YOLO format: - <code>data.yaml</code> or <code>data.yml</code> exists at the root - <code>train/images/</code> directory exists with images</p>"},{"location":"learn/train/dataset-formats/#empty-annotations","title":"Empty Annotations","text":"<p>If images have no objects, handle them as follows:</p> <p>COCO format: Include the image in the <code>images</code> array but don't add any annotations for it.</p> <p>YOLO format: Create an empty <code>.txt</code> file (0 bytes) for the image, or omit the label file entirely.</p>"},{"location":"learn/train/dataset-formats/#class-id-mismatch","title":"Class ID Mismatch","text":"<p>COCO format: Category IDs in annotations must match IDs defined in the <code>categories</code> array.</p> <p>YOLO format: Class IDs in label files must be valid indices (0 to <code>nc-1</code>) based on the <code>names</code> list in <code>data.yaml</code>.</p>"},{"location":"learn/train/training-parameters/","title":"Training Parameters","text":"<p>This page provides a complete reference of all parameters available when training RF-DETR models.</p>"},{"location":"learn/train/training-parameters/#basic-example","title":"Basic Example","text":"<pre><code>from rfdetr import RFDETRMedium\n\nmodel = RFDETRMedium()\n\nmodel.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=\"output\"\n)\n</code></pre>"},{"location":"learn/train/training-parameters/#core-parameters","title":"Core Parameters","text":"<p>These are the essential parameters for training:</p> Parameter Type Default Description <code>dataset_dir</code> <code>str</code> Required Path to your dataset directory. RF-DETR auto-detects if it's in COCO or YOLO format. See Dataset Formats. <code>output_dir</code> <code>str</code> <code>\"output\"</code> Directory where training artifacts (checkpoints, logs) are saved. <code>epochs</code> <code>int</code> <code>100</code> Number of full passes over the training dataset. <code>batch_size</code> <code>int</code> <code>4</code> Number of samples processed per iteration. Higher values require more GPU memory. <code>grad_accum_steps</code> <code>int</code> <code>4</code> Accumulates gradients over multiple mini-batches. Use with <code>batch_size</code> to achieve effective batch size. <code>resume</code> <code>str</code> <code>None</code> Path to a saved checkpoint to continue training. Restores model weights, optimizer state, and scheduler."},{"location":"learn/train/training-parameters/#understanding-batch-size","title":"Understanding Batch Size","text":"<p>The effective batch size is calculated as:</p> <pre><code>effective_batch_size = batch_size \u00d7 grad_accum_steps \u00d7 num_gpus\n</code></pre> <p>Recommended configurations for different GPUs (targeting effective batch size of 16):</p> GPU VRAM <code>batch_size</code> <code>grad_accum_steps</code> A100 40-80GB 16 1 RTX 4090 24GB 8 2 RTX 3090 24GB 8 2 T4 16GB 4 4 RTX 3070 8GB 2 8"},{"location":"learn/train/training-parameters/#learning-rate-parameters","title":"Learning Rate Parameters","text":"Parameter Type Default Description <code>lr</code> <code>float</code> <code>1e-4</code> Learning rate for most parts of the model. <code>lr_encoder</code> <code>float</code> <code>1.5e-4</code> Learning rate specifically for the backbone encoder. Can be set lower than <code>lr</code> if you want to fine-tune the encoder more conservatively than the rest of the model. <p>Learning rate tips</p> <ul> <li>Start with the default values for fine-tuning</li> <li>If the model doesn't converge, try reducing <code>lr</code> by half</li> <li>For training from scratch (not recommended), you may need higher learning rates</li> </ul>"},{"location":"learn/train/training-parameters/#resolution-parameters","title":"Resolution Parameters","text":"Parameter Type Default Description <code>resolution</code> <code>int</code> Model-dependent Input image resolution. Higher values can improve accuracy but require more memory. Must be divisible by 56. <p>Common resolution values:</p> Resolution Memory Usage Use Case 560 Low Small objects, limited GPU memory 672 Medium Balanced (default for many models) 784 High High accuracy requirements 896 Very High Maximum quality (requires large GPU)"},{"location":"learn/train/training-parameters/#regularization-parameters","title":"Regularization Parameters","text":"Parameter Type Default Description <code>weight_decay</code> <code>float</code> <code>1e-4</code> L2 regularization coefficient. Helps prevent overfitting by penalizing large weights."},{"location":"learn/train/training-parameters/#hardware-parameters","title":"Hardware Parameters","text":"Parameter Type Default Description <code>device</code> <code>str</code> <code>\"cuda\"</code> Device to run training on. Options: <code>\"cuda\"</code>, <code>\"cpu\"</code>, <code>\"mps\"</code> (Apple Silicon). <code>gradient_checkpointing</code> <code>bool</code> <code>False</code> Re-computes parts of the forward pass during backpropagation to reduce memory usage. Lowers memory needs but increases training time."},{"location":"learn/train/training-parameters/#ema-exponential-moving-average","title":"EMA (Exponential Moving Average)","text":"Parameter Type Default Description <code>use_ema</code> <code>bool</code> <code>True</code> Enables Exponential Moving Average of weights. Produces a smoothed checkpoint that often improves final performance. <p>What is EMA?</p> <p>EMA maintains a moving average of the model weights throughout training. This smoothed version often generalizes better than the raw weights and is commonly used for the final model.</p>"},{"location":"learn/train/training-parameters/#checkpoint-parameters","title":"Checkpoint Parameters","text":"Parameter Type Default Description <code>checkpoint_interval</code> <code>int</code> <code>10</code> Frequency (in epochs) at which model checkpoints are saved. More frequent saves provide better coverage but consume more storage."},{"location":"learn/train/training-parameters/#checkpoint-files","title":"Checkpoint Files","text":"<p>During training, multiple checkpoints are saved:</p> File Description <code>checkpoint.pth</code> Most recent checkpoint (for resuming) <code>checkpoint_&lt;N&gt;.pth</code> Periodic checkpoint at epoch N <code>checkpoint_best_ema.pth</code> Best validation performance (EMA weights) <code>checkpoint_best_regular.pth</code> Best validation performance (raw weights) <code>checkpoint_best_total.pth</code> Final best model for inference"},{"location":"learn/train/training-parameters/#early-stopping-parameters","title":"Early Stopping Parameters","text":"Parameter Type Default Description <code>early_stopping</code> <code>bool</code> <code>False</code> Enable early stopping based on validation mAP. <code>early_stopping_patience</code> <code>int</code> <code>10</code> Number of epochs without improvement before stopping. <code>early_stopping_min_delta</code> <code>float</code> <code>0.001</code> Minimum change in mAP to qualify as an improvement. <code>early_stopping_use_ema</code> <code>bool</code> <code>False</code> Whether to track improvements using EMA model metrics."},{"location":"learn/train/training-parameters/#early-stopping-example","title":"Early Stopping Example","text":"<pre><code>model.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=200,\n    batch_size=4,\n    early_stopping=True,\n    early_stopping_patience=15,\n    early_stopping_min_delta=0.005\n)\n</code></pre> <p>This configuration will: - Train for up to 200 epochs - Stop early if mAP doesn't improve by at least 0.005 for 15 consecutive epochs</p>"},{"location":"learn/train/training-parameters/#logging-parameters","title":"Logging Parameters","text":"Parameter Type Default Description <code>tensorboard</code> <code>bool</code> <code>True</code> Enable TensorBoard logging. Requires <code>pip install \"rfdetr[metrics]\"</code>. <code>wandb</code> <code>bool</code> <code>False</code> Enable Weights &amp; Biases logging. Requires <code>pip install \"rfdetr[metrics]\"</code>. <code>project</code> <code>str</code> <code>None</code> Project name for W&amp;B logging. <code>run</code> <code>str</code> <code>None</code> Run name for W&amp;B logging. If not specified, W&amp;B assigns a random name."},{"location":"learn/train/training-parameters/#logging-example","title":"Logging Example","text":"<pre><code>model.train(\n    dataset_dir=\"path/to/dataset\",\n    epochs=100,\n    tensorboard=True,\n    wandb=True,\n    project=\"my-detection-project\",\n    run=\"experiment-001\"\n)\n</code></pre>"},{"location":"learn/train/training-parameters/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<p>Below is a summary table of all training parameters:</p> Parameter Type Default Description <code>dataset_dir</code> str Required Path to COCO or YOLO formatted dataset with train/valid/test splits. <code>output_dir</code> str \"output\" Directory for checkpoints, logs, and other training artifacts. <code>epochs</code> int 100 Number of full passes over the dataset. <code>batch_size</code> int 4 Samples per iteration. Balance with <code>grad_accum_steps</code>. <code>grad_accum_steps</code> int 4 Gradient accumulation steps for effective larger batch sizes. <code>lr</code> float 1e-4 Learning rate for the model (excluding encoder). <code>lr_encoder</code> float 1.5e-4 Learning rate for the backbone encoder. <code>resolution</code> int Model-specific Input image size (must be divisible by 56). <code>weight_decay</code> float 1e-4 L2 regularization coefficient. <code>device</code> str \"cuda\" Training device: cuda, cpu, or mps. <code>use_ema</code> bool True Enable Exponential Moving Average of weights. <code>gradient_checkpointing</code> bool False Trade compute for memory during backprop. <code>checkpoint_interval</code> int 10 Save checkpoint every N epochs. <code>resume</code> str None Path to checkpoint for resuming training. <code>tensorboard</code> bool True Enable TensorBoard logging. <code>wandb</code> bool False Enable Weights &amp; Biases logging. <code>project</code> str None W&amp;B project name. <code>run</code> str None W&amp;B run name. <code>early_stopping</code> bool False Enable early stopping. <code>early_stopping_patience</code> int 10 Epochs without improvement before stopping. <code>early_stopping_min_delta</code> float 0.001 Minimum mAP change to qualify as improvement. <code>early_stopping_use_ema</code> bool False Use EMA model for early stopping metrics."},{"location":"reference/2xlarge/","title":"RF-DETR 2XLarge","text":"<p>License Notice</p> <p>This model is licensed under the Platform Model License (PML-1.0). See <code>LICENSE.platform</code> for details.</p> <p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/platform/models.py</code> <pre><code>class RFDETR2XLarge(RFDETR):\n    size = \"rfdetr-2xlarge\"\n\n    def __init__(self, accept_platform_model_license: bool = False, **kwargs):\n        if accept_platform_model_license is not True:\n            raise ValueError(\n                \"You must accept the platform model license (LICENSE.platform) to use this model. \"\n                \"You can do this by setting accept_platform_model_license=True when initializing the model.\"\n            )\n        super().__init__(**kwargs)\n\n    def get_model_config(self, **kwargs):\n        return RFDETR2XLargeConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge-attributes","title":"Attributes","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge-functions","title":"Functions","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/2xlarge/#rfdetr.platform.models.RFDETR2XLarge.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/base/","title":"RF-DETR Base (Deprecated)","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Base model (29M parameters).</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRBase(RFDETR):\n    \"\"\"\n    Train an RF-DETR Base model (29M parameters).\n    \"\"\"\n    size = \"rfdetr-base\"\n    def get_model_config(self, **kwargs):\n        return RFDETRBaseConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase-attributes","title":"Attributes","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase-functions","title":"Functions","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/large/","title":"RF-DETR Large","text":"<p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRLarge(RFDETR):\n    size = \"rfdetr-large\"\n    def __init__(self, **kwargs):\n        self.init_error = None\n        self.is_deprecated = False\n        try:\n            super().__init__(**kwargs)\n        except Exception as e:\n            self.init_error = e\n            self.is_deprecated = True\n            try:\n                super().__init__(**kwargs)\n                logger.warning(\n                    \"\\n\"\n                    \"=\"*100 + \"\\n\"\n                    \"WARNING: Automatically switched to deprecated model configuration, due to using deprecated weights. \"\n                    \"This will be removed in a future version.\\n\"\n                    \"Please retrain your model with the new weights and configuration.\\n\"\n                    \"=\"*100 + \"\\n\"\n                )\n            except Exception:\n                raise self.init_error\n\n    def get_model_config(self, **kwargs):\n        if not self.is_deprecated:\n            return RFDETRLargeConfig(**kwargs)\n        else:\n            return RFDETRLargeDeprecatedConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge-attributes","title":"Attributes","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge-functions","title":"Functions","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/large_deprecated/","title":"RF-DETR Large (Deprecated)","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Large model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRLargeDeprecated(RFDETR):\n    \"\"\"\n    Train an RF-DETR Large model.\n    \"\"\"\n    size = \"rfdetr-large\"\n    def __init__(self, **kwargs):\n        warnings.warn(\n    \"RFDETRLargeDeprecated is deprecated and will be removed in a future version. \"\n    \"Please use RFDETRLarge instead.\",\n    category=DeprecationWarning,\n    stacklevel=2\n)\n        super().__init__(**kwargs)\n\n    def get_model_config(self, **kwargs):\n        return RFDETRLargeDeprecatedConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated-attributes","title":"Attributes","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated-functions","title":"Functions","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/large_deprecated/#rfdetr.detr.RFDETRLargeDeprecated.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/medium/","title":"RF-DETR Medium","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Medium model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRMedium(RFDETR):\n    \"\"\"\n    Train an RF-DETR Medium model.\n    \"\"\"\n    size = \"rfdetr-medium\"\n    def get_model_config(self, **kwargs):\n        return RFDETRMediumConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium-attributes","title":"Attributes","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium-functions","title":"Functions","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/nano/","title":"RF-DETR Nano","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Nano model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRNano(RFDETR):\n    \"\"\"\n    Train an RF-DETR Nano model.\n    \"\"\"\n    size = \"rfdetr-nano\"\n    def get_model_config(self, **kwargs):\n        return RFDETRNanoConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano-attributes","title":"Attributes","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano-functions","title":"Functions","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/rfdetr/","title":"RF-DETR","text":"<p>The base RF-DETR class implements the core methods for training RF-DETR models, running inference on the models, optimising models, and uploading trained models for deployment.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETR:\n    \"\"\"\n    The base RF-DETR class implements the core methods for training RF-DETR models,\n    running inference on the models, optimising models, and uploading trained\n    models for deployment.\n    \"\"\"\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n    size = None\n\n    def __init__(self, **kwargs):\n        self.model_config = self.get_model_config(**kwargs)\n        self.maybe_download_pretrain_weights()\n        self.model = self.get_model(self.model_config)\n        self.callbacks = defaultdict(list)\n\n        self.model.inference_model = None\n        self._is_optimized_for_inference = False\n        self._has_warned_about_not_being_optimized_for_inference = False\n        self._optimized_has_been_compiled = False\n        self._optimized_batch_size = None\n        self._optimized_resolution = None\n        self._optimized_dtype = None\n\n    def maybe_download_pretrain_weights(self):\n        \"\"\"\n        Download pre-trained weights if they are not already downloaded.\n        \"\"\"\n        download_pretrain_weights(self.model_config.pretrain_weights)\n\n    def get_model_config(self, **kwargs):\n        \"\"\"\n        Retrieve the configuration parameters used by the model.\n        \"\"\"\n        return ModelConfig(**kwargs)\n\n    def train(self, **kwargs):\n        \"\"\"\n        Train an RF-DETR model.\n        \"\"\"\n        config = self.get_train_config(**kwargs)\n        self.train_from_config(config, **kwargs)\n\n    def optimize_for_inference(self, compile=True, batch_size=1, dtype=torch.float32):\n        self.remove_optimized_model()\n\n        self.model.inference_model = deepcopy(self.model.model)\n        self.model.inference_model.eval()\n        self.model.inference_model.export()\n\n        self._optimized_resolution = self.model.resolution\n        self._is_optimized_for_inference = True\n\n        self.model.inference_model = self.model.inference_model.to(dtype=dtype)\n        self._optimized_dtype = dtype\n\n        if compile:\n            self.model.inference_model = torch.jit.trace(\n                self.model.inference_model,\n                torch.randn(\n                    batch_size, 3, self.model.resolution, self.model.resolution,\n                    device=self.model.device,\n                    dtype=dtype\n                )\n            )\n            self._optimized_has_been_compiled = True\n            self._optimized_batch_size = batch_size\n\n    def remove_optimized_model(self):\n        self.model.inference_model = None\n        self._is_optimized_for_inference = False\n        self._optimized_has_been_compiled = False\n        self._optimized_batch_size = None\n        self._optimized_resolution = None\n        self._optimized_half = False\n\n    def export(self, **kwargs):\n        \"\"\"\n        Export your model to an ONNX file.\n\n        See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n        \"\"\"\n        self.model.export(**kwargs)\n\n    @staticmethod\n    def _load_classes(dataset_dir) -&gt; List[str]:\n        \"\"\"Load class names from a COCO or YOLO dataset directory.\"\"\"\n        if is_valid_coco_dataset(dataset_dir):\n            coco_path = os.path.join(dataset_dir, \"train\", \"_annotations.coco.json\")\n            with open(coco_path, \"r\") as f:\n                anns = json.load(f)\n            class_names = [c[\"name\"] for c in anns[\"categories\"] if c[\"supercategory\"] != \"none\"]\n            return class_names\n\n        # list all YAML files in the folder\n        if is_valid_yolo_dataset(dataset_dir):\n            yaml_paths = glob.glob(os.path.join(dataset_dir, \"*.yaml\")) + glob.glob(os.path.join(dataset_dir, \"*.yml\"))\n            # any YAML file starting with data e.g. data.yaml, dataset.yaml\n            yaml_data_files = [yp for yp in yaml_paths if os.path.basename(yp).startswith(\"data\")]\n            yaml_path = yaml_data_files[0]\n            with open(yaml_path, \"r\") as f:\n                data = yaml.safe_load(f)\n            if \"names\" in data:\n                if isinstance(data[\"names\"], dict):\n                    return [data[\"names\"][i] for i in sorted(data[\"names\"].keys())]\n                return data[\"names\"]\n            else:\n                raise ValueError(f\"Found {yaml_path} but it does not contain 'names' field.\")\n\n        raise FileNotFoundError(\n            f\"Could not find class names in {dataset_dir}. \"\n            \"Checked for COCO (train/_annotations.coco.json) and YOLO (data.yaml, data.yml) styles.\"\n        )\n\n    def train_from_config(self, config: TrainConfig, **kwargs):\n        if config.dataset_file == \"roboflow\":\n            class_names = self._load_classes(config.dataset_dir)\n            num_classes = len(class_names) + 1\n            self.model.class_names = class_names\n        elif config.dataset_file == \"yolo\":\n            class_names = self._load_classes(config.dataset_dir)\n            num_classes = len(class_names)\n            self.model.class_names = class_names\n        elif config.dataset_file == \"coco\":\n            class_names = COCO_CLASSES\n            num_classes = 90\n        else:\n            raise ValueError(f\"Invalid dataset file: {config.dataset_file}\")\n\n        if self.model_config.num_classes != num_classes:\n            logger.warning(\n                f\"Reinitializing your detection head with {num_classes} classes.\"\n            )\n            self.model.reinitialize_detection_head(num_classes)\n\n        train_config = config.dict()\n        model_config = self.model_config.dict()\n        model_config.pop(\"num_classes\")\n        if \"class_names\" in model_config:\n            model_config.pop(\"class_names\")\n\n        if \"class_names\" in train_config and train_config[\"class_names\"] is None:\n            train_config[\"class_names\"] = class_names\n\n        for k, v in train_config.items():\n            if k in model_config:\n                model_config.pop(k)\n            if k in kwargs:\n                kwargs.pop(k)\n\n        all_kwargs = {**model_config, **train_config, **kwargs, \"num_classes\": num_classes}\n\n        metrics_plot_sink = MetricsPlotSink(output_dir=config.output_dir)\n        self.callbacks[\"on_fit_epoch_end\"].append(metrics_plot_sink.update)\n        self.callbacks[\"on_train_end\"].append(metrics_plot_sink.save)\n\n        if config.tensorboard:\n            metrics_tensor_board_sink = MetricsTensorBoardSink(output_dir=config.output_dir)\n            self.callbacks[\"on_fit_epoch_end\"].append(metrics_tensor_board_sink.update)\n            self.callbacks[\"on_train_end\"].append(metrics_tensor_board_sink.close)\n\n        if config.wandb:\n            metrics_wandb_sink = MetricsWandBSink(\n                output_dir=config.output_dir,\n                project=config.project,\n                run=config.run,\n                config=config.model_dump()\n            )\n            self.callbacks[\"on_fit_epoch_end\"].append(metrics_wandb_sink.update)\n            self.callbacks[\"on_train_end\"].append(metrics_wandb_sink.close)\n\n        if config.early_stopping:\n            from rfdetr.util.early_stopping import EarlyStoppingCallback\n            early_stopping_callback = EarlyStoppingCallback(\n                model=self.model,\n                patience=config.early_stopping_patience,\n                min_delta=config.early_stopping_min_delta,\n                use_ema=config.early_stopping_use_ema,\n                segmentation_head=config.segmentation_head\n            )\n            self.callbacks[\"on_fit_epoch_end\"].append(early_stopping_callback.update)\n\n        self.model.train(\n            **all_kwargs,\n            callbacks=self.callbacks,\n        )\n\n    def get_train_config(self, **kwargs):\n        \"\"\"\n        Retrieve the configuration parameters that will be used for training.\n        \"\"\"\n        return TrainConfig(**kwargs)\n\n    def get_model(self, config: ModelConfig):\n        \"\"\"\n        Retrieve a model instance based on the provided configuration.\n        \"\"\"\n        return Model(**config.dict())\n\n    # Get class_names from the model\n    @property\n    def class_names(self):\n        \"\"\"\n        Retrieve the class names supported by the loaded model.\n\n        Returns:\n            dict: A dictionary mapping class IDs to class names. The keys are integers starting from\n        \"\"\"\n        if hasattr(self.model, 'class_names') and self.model.class_names:\n            return {i+1: name for i, name in enumerate(self.model.class_names)}\n\n        return COCO_CLASSES\n\n    def predict(\n        self,\n        images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n        threshold: float = 0.5,\n        **kwargs,\n    ) -&gt; Union[sv.Detections, List[sv.Detections]]:\n        \"\"\"Performs object detection on the input images and returns bounding box\n        predictions.\n\n        This method accepts a single image or a list of images in various formats\n        (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n        RGB channel order. If a torch.Tensor is provided, it must already be normalized\n        to values in the [0, 1] range and have the shape (C, H, W).\n\n        Args:\n            images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n                A single image or a list of images to process. Images can be provided\n                as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n            threshold (float, optional):\n                The minimum confidence score needed to consider a detected bounding box valid.\n            **kwargs:\n                Additional keyword arguments.\n\n        Returns:\n            Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n                objects, each containing bounding box coordinates, confidence scores,\n                and class IDs.\n        \"\"\"\n        if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n            logger.warning(\n                \"Model is not optimized for inference. \"\n                \"Latency may be higher than expected. \"\n                \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n            )\n            self._has_warned_about_not_being_optimized_for_inference = True\n\n            self.model.model.eval()\n\n        if not isinstance(images, list):\n            images = [images]\n\n        orig_sizes = []\n        processed_images = []\n\n        for img in images:\n\n            if isinstance(img, str):\n                img = Image.open(img)\n\n            if not isinstance(img, torch.Tensor):\n                img = F.to_tensor(img)\n\n            if (img &gt; 1).any():\n                raise ValueError(\n                    \"Image has pixel values above 1. Please ensure the image is \"\n                    \"normalized (scaled to [0, 1]).\"\n                )\n            if img.shape[0] != 3:\n                raise ValueError(\n                    f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                    f\"{img.shape[0]} channels.\"\n                )\n            img_tensor = img\n\n            h, w = img_tensor.shape[1:]\n            orig_sizes.append((h, w))\n\n            img_tensor = img_tensor.to(self.model.device)\n            img_tensor = F.normalize(img_tensor, self.means, self.stds)\n            img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n            processed_images.append(img_tensor)\n\n        batch_tensor = torch.stack(processed_images)\n\n        if self._is_optimized_for_inference:\n            if self._optimized_resolution != batch_tensor.shape[2]:\n                # this could happen if someone manually changes self.model.resolution after optimizing the model\n                raise ValueError(f\"Resolution mismatch. \"\n                                 f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                                 f\"but got {batch_tensor.shape[2]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n            if self._optimized_has_been_compiled:\n                if self._optimized_batch_size != batch_tensor.shape[0]:\n                    raise ValueError(f\"Batch size mismatch. \"\n                                     f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                     f\"but got {batch_tensor.shape[0]}. \"\n                                     \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                     \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                     \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n        with torch.no_grad():\n            if self._is_optimized_for_inference:\n                predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n            else:\n                predictions = self.model.model(batch_tensor)\n            if isinstance(predictions, tuple):\n                return_predictions = {\n                    \"pred_logits\": predictions[1],\n                    \"pred_boxes\": predictions[0],\n                }\n                if len(predictions) == 3:\n                    return_predictions[\"pred_masks\"] = predictions[2]\n                predictions = return_predictions\n            target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n            results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n        detections_list = []\n        for result in results:\n            scores = result[\"scores\"]\n            labels = result[\"labels\"]\n            boxes = result[\"boxes\"]\n\n            keep = scores &gt; threshold\n            scores = scores[keep]\n            labels = labels[keep]\n            boxes = boxes[keep]\n\n            if \"masks\" in result:\n                masks = result[\"masks\"]\n                masks = masks[keep]\n\n                detections = sv.Detections(\n                    xyxy=boxes.float().cpu().numpy(),\n                    confidence=scores.float().cpu().numpy(),\n                    class_id=labels.cpu().numpy(),\n                    mask=masks.squeeze(1).cpu().numpy(),\n                )\n            else:\n                detections = sv.Detections(\n                    xyxy=boxes.float().cpu().numpy(),\n                    confidence=scores.float().cpu().numpy(),\n                    class_id=labels.cpu().numpy(),\n                )\n\n            detections_list.append(detections)\n\n        return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n\n    def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n        \"\"\"\n        Deploy the trained RF-DETR model to Roboflow.\n\n        Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n        You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n        Args:\n            workspace (str): The name of the Roboflow workspace to deploy to.\n            project_ids (List[str]): A list of project IDs to which the model will be deployed\n            api_key (str, optional): Your Roboflow API key. If not provided,\n                it will be read from the environment variable `ROBOFLOW_API_KEY`.\n            size (str, optional): The size of the model to deploy. If not provided,\n                it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n            model_name (str, optional): The name you want to give the uploaded model.\n            If not provided, it will default to \"&lt;size&gt;-uploaded\".\n        Raises:\n            ValueError: If the `api_key` is not provided and not found in the environment\n                variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n        \"\"\"\n        import shutil\n\n        from roboflow import Roboflow\n        if api_key is None:\n            api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n            if api_key is None:\n                raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n        rf = Roboflow(api_key=api_key)\n        workspace = rf.workspace(workspace)\n\n        if self.size is None and size is None:\n            raise ValueError(\"Must set size for custom architectures\")\n\n        size = self.size or size\n        tmp_out_dir = \".roboflow_temp_upload\"\n        os.makedirs(tmp_out_dir, exist_ok=True)\n        outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n        torch.save(\n            {\n                \"model\": self.model.model.state_dict(),\n                \"args\": self.model.args\n            }, outpath\n        )\n        project = workspace.project(project_id)\n        version = project.version(version)\n        version.deploy(\n            model_type=size,\n            model_path=tmp_out_dir,\n            filename=\"weights.pt\"\n        )\n        shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR-attributes","title":"Attributes","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR-functions","title":"Functions","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.get_model_config","title":"<code>get_model_config(**kwargs)</code>","text":"<p>Retrieve the configuration parameters used by the model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model_config(self, **kwargs):\n    \"\"\"\n    Retrieve the configuration parameters used by the model.\n    \"\"\"\n    return ModelConfig(**kwargs)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.get_train_config","title":"<code>get_train_config(**kwargs)</code>","text":"<p>Retrieve the configuration parameters that will be used for training.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_train_config(self, **kwargs):\n    \"\"\"\n    Retrieve the configuration parameters that will be used for training.\n    \"\"\"\n    return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/seg_2xlarge/","title":"RF-DETR Seg 2XLarge","text":"<p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSeg2XLarge(RFDETR):\n    size = \"rfdetr-seg-2xlarge\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSeg2XLargeConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return SegmentationTrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge-attributes","title":"Attributes","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge-functions","title":"Functions","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/seg_2xlarge/#rfdetr.detr.RFDETRSeg2XLarge.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/seg_large/","title":"RF-DETR Seg Large","text":"<p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSegLarge(RFDETR):\n    size = \"rfdetr-seg-large\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSegLargeConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return SegmentationTrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge-attributes","title":"Attributes","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge-functions","title":"Functions","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/seg_large/#rfdetr.detr.RFDETRSegLarge.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/seg_medium/","title":"RF-DETR Seg Medium","text":"<p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSegMedium(RFDETR):\n    size = \"rfdetr-seg-medium\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSegMediumConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return SegmentationTrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium-attributes","title":"Attributes","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium-functions","title":"Functions","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/seg_medium/#rfdetr.detr.RFDETRSegMedium.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/seg_nano/","title":"RF-DETR Seg Nano","text":"<p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSegNano(RFDETR):\n    size = \"rfdetr-seg-nano\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSegNanoConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return SegmentationTrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano-attributes","title":"Attributes","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano-functions","title":"Functions","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/seg_nano/#rfdetr.detr.RFDETRSegNano.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/seg_preview/","title":"RF-DETR Seg Preview (Deprecated)","text":"<p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSegPreview(RFDETR):\n    size = \"rfdetr-seg-preview\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSegPreviewConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return SegmentationTrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview-attributes","title":"Attributes","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview-functions","title":"Functions","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/seg_preview/#rfdetr.detr.RFDETRSegPreview.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/seg_small/","title":"RF-DETR Seg Small","text":"<p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSegSmall(RFDETR):\n    size = \"rfdetr-seg-small\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSegSmallConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return SegmentationTrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall-attributes","title":"Attributes","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall-functions","title":"Functions","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/seg_small/#rfdetr.detr.RFDETRSegSmall.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/seg_xlarge/","title":"RF-DETR Seg XLarge","text":"<p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSegXLarge(RFDETR):\n    size = \"rfdetr-seg-xlarge\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSegXLargeConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return SegmentationTrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge-attributes","title":"Attributes","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge-functions","title":"Functions","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/seg_xlarge/#rfdetr.detr.RFDETRSegXLarge.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/segmentation_train_config/","title":"Segmentation Train Config","text":"<p>               Bases: <code>TrainConfig</code></p> Source code in <code>rfdetr/config.py</code> <pre><code>class SegmentationTrainConfig(TrainConfig):\n    mask_point_sample_ratio: int = 16\n    mask_ce_loss_coef: float = 5.0\n    mask_dice_loss_coef: float = 5.0\n    cls_loss_coef: float = 5.0\n    segmentation_head: bool = True\n</code></pre>"},{"location":"reference/segmentation_train_config/#rfdetr.config.SegmentationTrainConfig-functions","title":"Functions","text":""},{"location":"reference/segmentation_train_config/#rfdetr.config.SegmentationTrainConfig.expand_paths","title":"<code>expand_paths(v)</code>  <code>classmethod</code>","text":"<p>Expand user paths (e.g., '~' or paths with separators) but leave simple filenames (like 'rf-detr-base.pth') unchanged so they can match hosted model keys.</p> Source code in <code>rfdetr/config.py</code> <pre><code>@field_validator(\"dataset_dir\", \"output_dir\", mode=\"after\")\n@classmethod\ndef expand_paths(cls, v: str) -&gt; str:\n    \"\"\"\n    Expand user paths (e.g., '~' or paths with separators) but leave simple filenames\n    (like 'rf-detr-base.pth') unchanged so they can match hosted model keys.\n    \"\"\"\n    if v is None:\n        return v\n    return os.path.realpath(os.path.expanduser(v))\n</code></pre>"},{"location":"reference/small/","title":"RF-DETR Small","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Small model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSmall(RFDETR):\n    \"\"\"\n    Train an RF-DETR Small model.\n    \"\"\"\n    size = \"rfdetr-small\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSmallConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall-attributes","title":"Attributes","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall-functions","title":"Functions","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/train_config/","title":"Train Config","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>rfdetr/config.py</code> <pre><code>class TrainConfig(BaseModel):\n    lr: float = 1e-4\n    lr_encoder: float = 1.5e-4\n    batch_size: int = 4\n    grad_accum_steps: int = 4\n    epochs: int = 100\n    resume: Optional[str] = None\n    ema_decay: float = 0.993\n    ema_tau: int = 100\n    lr_drop: int = 100\n    checkpoint_interval: int = 10\n    warmup_epochs: float = 0.0\n    lr_vit_layer_decay: float = 0.8\n    lr_component_decay: float = 0.7\n    drop_path: float = 0.0\n    group_detr: int = 13\n    ia_bce_loss: bool = True\n    cls_loss_coef: float = 1.0\n    num_select: int = 300\n    dataset_file: Literal[\"coco\", \"o365\", \"roboflow\", \"yolo\"] = \"roboflow\"\n    square_resize_div_64: bool = True\n    dataset_dir: str\n    output_dir: str = \"output\"\n    multi_scale: bool = True\n    expanded_scales: bool = True\n    do_random_resize_via_padding: bool = False\n    use_ema: bool = True\n    num_workers: int = 2\n    weight_decay: float = 1e-4\n    early_stopping: bool = False\n    early_stopping_patience: int = 10\n    early_stopping_min_delta: float = 0.001\n    early_stopping_use_ema: bool = False\n    tensorboard: bool = True\n    wandb: bool = False\n    project: Optional[str] = None\n    run: Optional[str] = None\n    class_names: List[str] = None\n    run_test: bool = True\n    segmentation_head: bool = False\n    eval_max_dets: int = 500\n\n    @field_validator(\"dataset_dir\", \"output_dir\", mode=\"after\")\n    @classmethod\n    def expand_paths(cls, v: str) -&gt; str:\n        \"\"\"\n        Expand user paths (e.g., '~' or paths with separators) but leave simple filenames\n        (like 'rf-detr-base.pth') unchanged so they can match hosted model keys.\n        \"\"\"\n        if v is None:\n            return v\n        return os.path.realpath(os.path.expanduser(v))\n</code></pre>"},{"location":"reference/train_config/#rfdetr.config.TrainConfig-functions","title":"Functions","text":""},{"location":"reference/train_config/#rfdetr.config.TrainConfig.expand_paths","title":"<code>expand_paths(v)</code>  <code>classmethod</code>","text":"<p>Expand user paths (e.g., '~' or paths with separators) but leave simple filenames (like 'rf-detr-base.pth') unchanged so they can match hosted model keys.</p> Source code in <code>rfdetr/config.py</code> <pre><code>@field_validator(\"dataset_dir\", \"output_dir\", mode=\"after\")\n@classmethod\ndef expand_paths(cls, v: str) -&gt; str:\n    \"\"\"\n    Expand user paths (e.g., '~' or paths with separators) but leave simple filenames\n    (like 'rf-detr-base.pth') unchanged so they can match hosted model keys.\n    \"\"\"\n    if v is None:\n        return v\n    return os.path.realpath(os.path.expanduser(v))\n</code></pre>"},{"location":"reference/xlarge/","title":"RF-DETR XLarge","text":"<p>License Notice</p> <p>This model is licensed under the Platform Model License (PML-1.0). See <code>LICENSE.platform</code> for details.</p> <p>               Bases: <code>RFDETR</code></p> Source code in <code>rfdetr/platform/models.py</code> <pre><code>class RFDETRXLarge(RFDETR):\n    size = \"rfdetr-xlarge\"\n\n    def __init__(self, accept_platform_model_license: bool = False, **kwargs):\n        if accept_platform_model_license is not True:\n            raise ValueError(\n                \"You must accept the platform model license (LICENSE.platform) to use this model. \"\n                \"You can do this by setting accept_platform_model_license=True when initializing the model.\"\n            )\n        super().__init__(**kwargs)\n\n    def get_model_config(self, **kwargs):\n        return RFDETRXLargeConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge-attributes","title":"Attributes","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge-functions","title":"Functions","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    import shutil\n\n    from roboflow import Roboflow\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/export/) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.no_grad():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            return_predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0],\n            }\n            if len(predictions) == 3:\n                return_predictions[\"pred_masks\"] = predictions[2]\n            predictions = return_predictions\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocess(predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        if \"masks\" in result:\n            masks = result[\"masks\"]\n            masks = masks[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n                mask=masks.squeeze(1).cpu().numpy(),\n            )\n        else:\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/xlarge/#rfdetr.platform.models.RFDETRXLarge.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"}]}