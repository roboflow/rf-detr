{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RF-DETR: SOTA Real-Time Object Detection Model","text":""},{"location":"#introduction","title":"Introduction","text":"<p>RF-DETR is a real-time, transformer-based object detection model architecture developed by Roboflow and released under the Apache 2.0 license.</p> <p>RF-DETR is the first real-time model to exceed 60 AP on the Microsoft COCO benchmark alongside competitive performance at base sizes. It also achieves state-of-the-art performance on RF100-VL, an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is fastest and most accurate for its size when compared current real-time objection models.</p> <p>RF-DETR is small enough to run on the edge using Inference, making it an ideal model for deployments that need both strong accuracy and real-time performance.</p>"},{"location":"#results","title":"Results","text":"<p>We validated the performance of RF-DETR on both Microsoft COCO and the RF100-VL benchmarks.</p> <p>See our full benchmarks.</p> <p></p>"},{"location":"#install","title":"\ud83d\udcbb Install","text":"<p>You can install and use <code>rfdetr</code> in a Python&gt;=3.9 environment.</p> <p>Installation</p> pip (recommended)poetryuvrye <p> </p> <pre><code>pip install rfdetr\n</code></pre> <p> </p> <pre><code>poetry add rfdetr\n</code></pre> <p> </p> <pre><code>uv pip install rfdetr\n</code></pre> <p>For uv projects:</p> <pre><code>uv add rfdetr\n</code></pre> <p> </p> <pre><code>rye add rfdetr\n</code></pre> <p>git clone (for development)</p> virtualenvuv <pre><code># clone repository and navigate to root directory\ngit clone --depth 1 -b develop https://github.com/roboflow/rf-detr.git\ncd rf-detr\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\n\n# installation\npip install -e \".\"\n</code></pre> <pre><code># clone repository and navigate to root directory\ngit clone --depth 1 -b develop https://github.com/roboflow/rf-detr.git\ncd rf-detr\n\n# setup python environment and activate it\nuv venv\nsource .venv/bin/activate\n\n# installation\nuv pip install -r pyproject.toml -e . --all-extras\n</code></pre>"},{"location":"#quickstart","title":"\ud83d\ude80 Quickstart","text":"<ul> <li> <p>Run a Pre-Trained Model</p> <p>Load and run a pre-trained RF-DETR model.</p> <p> Tutorial</p> </li> <li> <p>Train an RF-DETR Model</p> <p>Learn how to train an RF-DETR model with the <code>rfdetr</code> Python package.</p> <p> Tutorial</p> </li> <li> <p>Deploy an RF-DETR Model</p> <p>Learn how to deploy an RF-DETR model in the cloud and on your device.</p> <p> Tutorial</p> </li> </ul>"},{"location":"learn/benchmarks/","title":"Benchmarks","text":"<p>RF-DETR is the first real-time model to exceed 60 AP on the Microsoft COCO benchmark alongside competitive performance at base sizes. It also achieves state-of-the-art performance on RF100-VL, an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is fastest and most accurate for its size when compared current real-time objection models.</p> <p>On this page, we outline our results from our Microsoft COCO benchmarks, benchmarks across all seven categories in the RF100-VL benchmark, and notes from our benchmarking.</p> <p>The table below shows the performance of RF-DETR, compared to other object detection models:</p> <p></p> Architecture COCO AP<sub>50</sub> COCO AP<sub>50:95</sub> RF100VL AP<sub>50</sub> RF100VL AP<sub>50:95</sub> Latency (ms) Params (M) RF-DETR-N 67.6 48.4 84.1 57.1 2.32 30.5 RF-DETR-S 72.1 53.0 85.9 59.6 3.52 32.1 RF-DETR-M 73.6 54.7 86.6 60.6 4.52 33.7 YOLO11-N 52.0 37.4 81.4 55.3 2.49 2.6 YOLO11-S 59.7 44.4 82.3 56.2 3.16 9.4 YOLO11-M 64.1 48.6 82.5 56.5 5.13 20.1 YOLO11-L 65.3 50.2 x x 6.65 25.3 YOLO11-X 66.5 51.2 x x 11.92 56.9 LW-DETR-T 60.7 42.9 x x 1.91 12.1 LW-DETR-S 66.8 48.0 84.5 58.0 2.62 14.6 LW-DETR-M 72.0 52.6 85.2 59.4 4.49 28.2 D-FINE-N 60.2 42.7 83.6 57.7 2.12 3.8 D-FINE-S 67.6 50.7 84.5 59.9 3.55 10.2 D-FINE-M 72.6 55.1 84.6 60.2 5.68 19.2 <p>We are actively working on RF-DETR Large and X-Large models using the same techniques we used to achieve the strong accuracy that RF-DETR Medium attains. This is why RF-DETR Large and X-Large is not yet reported on our pareto charts and why we haven't benchmarked other models at similar sizes. Check back in the next few weeks for the launch of new RF-DETR Large and X-Large models.</p>"},{"location":"learn/deploy/","title":"Deploy a Trained RF-DETR Model","text":"<p>You can deploy a fine-tuned RF-DETR model to Roboflow.</p> <p>Deploying to Roboflow allows you to create multi-step computer vision applications that run both in the cloud and your own hardware.</p> <p>To deploy your model to Roboflow, run:</p> <pre><code>from rfdetr import RFDETRNano\n\nx = RFDETRNano(pretrain_weights=\"&lt;path/to/pretrain/weights/dir&gt;\")\nx.deploy_to_roboflow(\n  workspace=\"&lt;your-workspace&gt;\",\n  project_id=\"&lt;your-project-id&gt;\",\n  version=1,\n  api_key=\"&lt;YOUR_API_KEY&gt;\"\n)\n</code></pre> <p>Above, set your Roboflow Workspace ID, the ID of the project to which you want to upload your model, and your Roboflow API key.</p> <ul> <li>Learn how to find your Workspace and Project ID.</li> <li>Learn how to find your API key.</li> </ul> <p>You can then run your model with Roboflow Inference:</p> <pre><code>import os\nimport supervision as sv\nfrom inference import get_model\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\nurl = \"https://media.roboflow.com/dog.jpeg\"\nimage = Image.open(BytesIO(requests.get(url).content))\n\nmodel = get_model(\"rfdetr-base\")  # replace with your Roboflow model ID\n\npredictions = model.infer(image, confidence=0.5)[0]\n\ndetections = sv.Detections.from_inference(predictions)\n\nlabels = [prediction.class_name for prediction in predictions.predictions]\n\nannotated_image = image.copy()\nannotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Above, replace <code>rfdetr-base</code> with the your Roboflow model ID. You can find this ID from the \"Models\" list in your Roboflow dashboard:</p> <p></p> <p>When you first run this model, your model weights will be cached for local use with Inference.</p> <p>You will then see the results from your fine-tuned model.</p>"},{"location":"learn/pretrained/","title":"Run a Pre-Trained Model","text":"<p>You can run any of the four supported RF-DETR base models -- Nano, Small, Medium, Large -- with Inference, an open source computer vision inference server. The base models are trained on the Microsoft COCO dataset.</p> Run on an ImageRun on a Video FileRun on a Webcam StreamRun on an RTSP Stream <p>To run RF-DETR on an image, use the following code:</p> <pre><code>import os\nimport supervision as sv\nfrom inference import get_model\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\nurl = \"https://media.roboflow.com/dog.jpeg\"\nimage = Image.open(BytesIO(requests.get(url).content))\n\nmodel = get_model(\"rfdetr-base\")\n\npredictions = model.infer(image, confidence=0.5)[0]\n\ndetections = sv.Detections.from_inference(predictions)\n\nlabels = [prediction.class_name for prediction in predictions.predictions]\n\nannotated_image = image.copy()\nannotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Above, replace the image URL with any image you want to use with the model.</p> <p>Here are the results from the code above:</p> <p> RF-DETR Base predictions </p> <p>To run RF-DETR on a video file, use the following code:</p> <pre><code>import supervision as sv\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRBase()\n\ndef callback(frame, index):\n    detections = model.predict(frame[:, :, ::-1], threshold=0.5)\n\n    labels = [\n        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n        for class_id, confidence\n        in zip(detections.class_id, detections.confidence)\n    ]\n\n    annotated_frame = frame.copy()\n    annotated_frame = sv.BoxAnnotator().annotate(annotated_frame, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n    return annotated_frame\n\nsv.process_video(\n    source_path=&lt;SOURCE_VIDEO_PATH&gt;,\n    target_path=&lt;TARGET_VIDEO_PATH&gt;,\n    callback=callback\n)\n</code></pre> <p>Above, set your <code>SOURCE_VIDEO_PATH</code> and <code>TARGET_VIDEO_PATH</code> to the directories of the video you want to process and where you want to save the results from inference, respectively.</p> <p>To run RF-DETR on a webcam input, use the following code:</p> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRBase()\n\ncap = cv2.VideoCapture(0)\nwhile True:\n    success, frame = cap.read()\n    if not success:\n        break\n\n    detections = model.predict(frame[:, :, ::-1], threshold=0.5)\n\n    labels = [\n        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n        for class_id, confidence\n        in zip(detections.class_id, detections.confidence)\n    ]\n\n    annotated_frame = frame.copy()\n    annotated_frame = sv.BoxAnnotator().annotate(annotated_frame, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"Webcam\", annotated_frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>To run RF-DETR on an RTSP stream, use the following code:</p> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRBase()\n\ncap = cv2.VideoCapture(&lt;RTSP_STREAM_URL&gt;)\nwhile True:\n    success, frame = cap.read()\n    if not success:\n        break\n\n    detections = model.predict(frame[:, :, ::-1], threshold=0.5)\n\n    labels = [\n        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n        for class_id, confidence\n        in zip(detections.class_id, detections.confidence)\n    ]\n\n    annotated_frame = frame.copy()\n    annotated_frame = sv.BoxAnnotator().annotate(annotated_frame, detections)\n    annotated_frame = sv.LabelAnnotator().annotate(annotated_frame, detections, labels)\n\n    cv2.imshow(\"RTSP Stream\", annotated_frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>You can change the RF-DETR model that the code snippet above uses. To do so, update <code>rfdetr-base</code> to any of the following values:</p> <ul> <li><code>rfdetr-nano</code></li> <li><code>rfdetr-small</code></li> <li><code>rfdetr-medium</code></li> <li><code>rfdetr-large</code></li> </ul>"},{"location":"learn/pretrained/#batch-inference","title":"Batch Inference","text":"<p>You can provide <code>.predict()</code> with either a single image or a list of images. When multiple images are supplied, they are processed together in a single forward pass, resulting in a corresponding list of detections.</p> <pre><code>import io\nimport requests\nimport supervision as sv\nfrom PIL import Image\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRBase()\n\nurls = [\n    \"https://media.roboflow.com/notebooks/examples/dog-2.jpeg\",\n    \"https://media.roboflow.com/notebooks/examples/dog-3.jpeg\"\n]\n\nimages = [Image.open(io.BytesIO(requests.get(url).content)) for url in urls]\n\ndetections_list = model.predict(images, threshold=0.5)\n\nfor image, detections in zip(images, detections_list):\n    labels = [\n        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n        for class_id, confidence\n        in zip(detections.class_id, detections.confidence)\n    ]\n\n    annotated_image = image.copy()\n    annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)\n    annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n\n    sv.plot_image(annotated_image)\n</code></pre>"},{"location":"learn/train/","title":"Train an RF-DETR Model","text":"<p>You can train an RF-DETR model on a custom dataset using the <code>rfdetr</code> Python package, or in the cloud using Roboflow.</p> <p>Training on device is ideal if you want to manage your training pipeline and have a GPU available for training.</p> <p>Training in the Roboflow Cloud is ideal if you want managed training whose weights you can deploy on your own hardware and with a hosted API.</p> <p>For this guide, we will train a model using the <code>rfdetr</code> Python package.</p> <p>Once you have trained a model with this guide, see our deploy an RF-DETR model guide to learn how to run inference with your model.</p>"},{"location":"learn/train/#dataset-structure","title":"Dataset structure","text":"<p>RF-DETR expects the dataset to be in COCO format. Divide your dataset into three subdirectories: <code>train</code>, <code>valid</code>, and <code>test</code>. Each subdirectory should contain its own <code>_annotations.coco.json</code> file that holds the annotations for that particular split, along with the corresponding image files. Below is an example of the directory structure:</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 _annotations.coco.json\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ... (other image files)\n\u251c\u2500\u2500 valid/\n\u2502   \u251c\u2500\u2500 _annotations.coco.json\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ... (other image files)\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 _annotations.coco.json\n    \u251c\u2500\u2500 image1.jpg\n    \u251c\u2500\u2500 image2.jpg\n    \u2514\u2500\u2500 ... (other image files)\n</code></pre> <p>Roboflow allows you to create object detection datasets from scratch or convert existing datasets from formats like YOLO, and then export them in COCO JSON format for training. You can also explore Roboflow Universe to find pre-labeled datasets for a range of use cases.</p>"},{"location":"learn/train/#fine-tuning","title":"Fine-tuning","text":"<p>You can fine-tune RF-DETR from pre-trained COCO checkpoints. By default, the RF-DETR-B checkpoint will be used. To get started quickly, please refer to our fine-tuning Google Colab notebook.</p> <pre><code>from rfdetr import RFDETRBase\n\nmodel = RFDETRBase()\n\nmodel.train(\n    dataset_dir=&lt;DATASET_PATH&gt;,\n    epochs=10,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=&lt;OUTPUT_PATH&gt;\n)\n</code></pre> <p>Different GPUs have different VRAM capacities, so adjust batch_size and grad_accum_steps to maintain a total batch size of 16. For example, on a powerful GPU like the A100, use <code>batch_size=16</code> and <code>grad_accum_steps=1</code>; on smaller GPUs like the T4, use <code>batch_size=4</code> and <code>grad_accum_steps=4</code>. This gradient accumulation strategy helps train effectively even with limited memory.</p> More parameters Parameter Description <code>dataset_dir</code> Specifies the COCO-formatted dataset location with <code>train</code>, <code>valid</code>, and <code>test</code> folders, each containing <code>_annotations.coco.json</code>. Ensures the model can properly read and parse data. <code>output_dir</code> Directory where training artifacts (checkpoints, logs, etc.) are saved. Important for experiment tracking and resuming training. <code>epochs</code> Number of full passes over the dataset. Increasing this can improve performance but extends total training time. <code>batch_size</code> Number of samples processed per iteration. Higher values require more GPU memory but can speed up training. Must be balanced with <code>grad_accum_steps</code> to maintain the intended total batch size. <code>grad_accum_steps</code> Accumulates gradients over multiple mini-batches, effectively raising the total batch size without requiring as much memory at once. Helps train on smaller GPUs at the cost of slightly more time per update. <code>lr</code> Learning rate for most parts of the model. Influences how quickly or cautiously the model adjusts its parameters. <code>lr_encoder</code> Learning rate specifically for the encoder portion of the model. Useful for fine-tuning encoder layers at a different pace. <code>resolution</code> Sets the input image dimensions. Higher values can improve accuracy but require more memory and can slow training. Must be divisible by 56. <code>weight_decay</code> Coefficient for L2 regularization. Helps prevent overfitting by penalizing large weights, often improving generalization. <code>device</code> Specifies the hardware (e.g., <code>cpu</code> or <code>cuda</code>) to run training on. GPU significantly speeds up training. <code>use_ema</code> Enables Exponential Moving Average of weights, producing a smoothed checkpoint. Often improves final performance with slight overhead. <code>gradient_checkpointing</code> Re-computes parts of the forward pass during backpropagation to reduce memory usage. Lowers memory needs but increases training time. <code>checkpoint_interval</code> Frequency (in epochs) at which model checkpoints are saved. More frequent saves provide better coverage but consume more storage. <code>resume</code> Path to a saved checkpoint for continuing training. Restores both model weights and optimizer state. <code>tensorboard</code> Enables logging of training metrics to TensorBoard for monitoring progress and performance. <code>wandb</code> Activates logging to Weights &amp; Biases, facilitating cloud-based experiment tracking and visualization. <code>project</code> Project name for Weights &amp; Biases logging. Groups multiple runs under a single heading. <code>run</code> Run name for Weights &amp; Biases logging, helping differentiate individual training sessions within a project. <code>early_stopping</code> Enables an early stopping callback that monitors mAP improvements to decide if training should be stopped. Helps avoid needless epochs when mAP plateaus. <code>early_stopping_patience</code> Number of consecutive epochs without mAP improvement before stopping. Prevents wasting resources on minimal gains. <code>early_stopping_min_delta</code> Minimum change in mAP to qualify as an improvement. Ensures that trivial gains don\u2019t reset the early stopping counter. <code>early_stopping_use_ema</code> Whether to track improvements using the EMA version of the model. Uses EMA metrics if available, otherwise falls back to regular mAP."},{"location":"learn/train/#result-checkpoints","title":"Result checkpoints","text":"<p>During training, multiple model checkpoints are saved to the output directory:</p> <ul> <li> <p><code>checkpoint.pth</code> \u2013 the most recent checkpoint, saved at the end of the latest epoch.</p> </li> <li> <p><code>checkpoint_&lt;number&gt;.pth</code> \u2013 periodic checkpoints saved every N epochs (default is every 10).</p> </li> <li> <p><code>checkpoint_best_ema.pth</code> \u2013 best checkpoint based on validation score, using the EMA (Exponential Moving Average) weights. EMA weights are a smoothed version of the model\u2019s parameters across training steps, often yielding better generalization.</p> </li> <li> <p><code>checkpoint_best_regular.pth</code> \u2013 best checkpoint based on validation score, using the raw (non-EMA) model weights.</p> </li> <li> <p><code>checkpoint_best_total.pth</code> \u2013 final checkpoint selected for inference and benchmarking. It contains only the model weights (no optimizer state or scheduler) and is chosen as the better of the EMA and non-EMA models based on validation performance.</p> </li> </ul> Checkpoint file sizes <p>Checkpoint sizes vary based on what they contain:</p> <ul> <li> <p>Training checkpoints (e.g. <code>checkpoint.pth</code>, <code>checkpoint_&lt;number&gt;.pth</code>) include model weights, optimizer state, scheduler state, and training metadata. Use these to resume training.</p> </li> <li> <p>Evaluation checkpoints (e.g. <code>checkpoint_best_ema.pth</code>, <code>checkpoint_best_regular.pth</code>) store only the model weights \u2014 either EMA or raw \u2014 and are used to track the best-performing models. These may come from different epochs depending on which version achieved the highest validation score.</p> </li> <li> <p>Stripped checkpoint (e.g. <code>checkpoint_best_total.pth</code>) contains only the final model weights and is optimized for inference and deployment.</p> </li> </ul>"},{"location":"learn/train/#resume-training","title":"Resume training","text":"<p>You can resume training from a previously saved checkpoint by passing the path to the <code>checkpoint.pth</code> file using the <code>resume</code> argument. This is useful when training is interrupted or you want to continue fine-tuning an already partially trained model. The training loop will automatically load the weights and optimizer state from the provided checkpoint file.</p> <pre><code>from rfdetr import RFDETRBase\n\nmodel = RFDETRBase()\n\nmodel.train(\n    dataset_dir=&lt;DATASET_PATH&gt;,\n    epochs=10,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=&lt;OUTPUT_PATH&gt;,\n    resume=&lt;CHECKPOINT_PATH&gt;\n)\n</code></pre>"},{"location":"learn/train/#early-stopping","title":"Early stopping","text":"<p>Early stopping monitors validation mAP and halts training if improvements remain below a threshold for a set number of epochs. This can reduce wasted computation once the model converges. Additional parameters\u2014such as <code>early_stopping_patience</code>, <code>early_stopping_min_delta</code>, and <code>early_stopping_use_ema</code>\u2014let you fine-tune the stopping behavior.</p> <pre><code>from rfdetr import RFDETRBase\n\nmodel = RFDETRBase()\n\nmodel.train(\n    dataset_dir=&lt;DATASET_PATH&gt;,\n    epochs=10,\n    batch_size=4\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=&lt;OUTPUT_PATH&gt;,\n    early_stopping=True\n)\n</code></pre>"},{"location":"learn/train/#multi-gpu-training","title":"Multi-GPU training","text":"<p>You can fine-tune RF-DETR on multiple GPUs using PyTorch\u2019s Distributed Data Parallel (DDP). Create a <code>main.py</code> script that initializes your model and calls <code>.train()</code> as usual than run it in terminal.</p> <pre><code>python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py\n</code></pre> <p>Replace <code>8</code> in the <code>--nproc_per_node argument</code> with the number of GPUs you want to use. This approach creates one training process per GPU and splits the workload automatically. Note that your effective batch size is multiplied by the number of GPUs, so you may need to adjust your <code>batch_size</code> and <code>grad_accum_steps</code> to maintain the same overall batch size.</p>"},{"location":"learn/train/#logging-with-tensorboard","title":"Logging with TensorBoard","text":"<p>TensorBoard is a powerful toolkit that helps you visualize and track training metrics. With TensorBoard set up, you can train your model and keep an eye on the logs to monitor performance, compare experiments, and optimize model training. To enable logging, simply pass <code>tensorboard=True</code> when training the model.</p> Using TensorBoard with RF-DETR   - TensorBoard logging requires additional packages. Install them with:      <pre><code>pip install \"rfdetr[metrics]\"\n</code></pre>  - To activate logging, pass the extra parameter `tensorboard=True` to `.train()`:      <pre><code>from rfdetr import RFDETRBase\n\nmodel = RFDETRBase()\n\nmodel.train(\n    dataset_dir=&lt;DATASET_PATH&gt;,\n    epochs=10,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=&lt;OUTPUT_PATH&gt;,\n    tensorboard=True\n)\n</code></pre>  - To use TensorBoard locally, navigate to your project directory and run:      <pre><code>tensorboard --logdir &lt;OUTPUT_DIR&gt;\n</code></pre>      Then open `http://localhost:6006/` in your browser to view your logs.  - To use TensorBoard in Google Colab run:      <pre><code>%load_ext tensorboard\n%tensorboard --logdir &lt;OUTPUT_DIR&gt;\n</code></pre>"},{"location":"learn/train/#logging-with-weights-and-biases","title":"Logging with Weights and Biases","text":"<p>Weights and Biases (W&amp;B) is a powerful cloud-based platform that helps you visualize and track training metrics. With W&amp;B set up, you can monitor performance, compare experiments, and optimize model training using its rich feature set. To enable logging, simply pass <code>wandb=True</code> when training the model.</p> Using Weights and Biases with RF-DETR   - Weights and Biases logging requires additional packages. Install them with:      <pre><code>pip install \"rfdetr[metrics]\"\n</code></pre>  - Before using W&amp;B, make sure you are logged in:      <pre><code>wandb login\n</code></pre>      You can retrieve your API key at wandb.ai/authorize.  - To activate logging, pass the extra parameter `wandb=True` to `.train()`:      <pre><code>from rfdetr import RFDETRBase\n\nmodel = RFDETRBase()\n\nmodel.train(\n    dataset_dir=&lt;DATASET_PATH&gt;,\n    epochs=10,\n    batch_size=4,\n    grad_accum_steps=4,\n    lr=1e-4,\n    output_dir=&lt;OUTPUT_PATH&gt;,\n    wandb=True,\n    project=&lt;PROJECT_NAME&gt;,\n    run=&lt;RUN_NAME&gt;\n)\n</code></pre>      In W&amp;B, projects are collections of related machine learning experiments, and runs are individual sessions where training or evaluation happens. If you don't specify a name for a run, W&amp;B will assign a random one automatically."},{"location":"learn/train/#load-and-run-fine-tuned-model","title":"Load and run fine-tuned model","text":"<pre><code>from rfdetr import RFDETRBase\n\nmodel = RFDETRBase(pretrain_weights=&lt;CHECKPOINT_PATH&gt;)\n\ndetections = model.predict(&lt;IMAGE_PATH&gt;)\n</code></pre>"},{"location":"learn/train/#onnx-export","title":"ONNX export","text":"<p>RF-DETR supports exporting models to the ONNX format, which enables interoperability with various inference frameworks and can improve deployment efficiency.</p> <p>To export your model, first install the <code>onnxexport</code> extension:</p> <pre><code>pip install rfdetr[onnxexport]\n</code></pre> <p>Then, run:</p> <pre><code>from rfdetr import RFDETRBase\n\nmodel = RFDETRBase(pretrain_weights=&lt;CHECKPOINT_PATH&gt;)\n\nmodel.export()\n</code></pre> <p>This command saves the ONNX model to the <code>output</code> directory.</p>"},{"location":"reference/base/","title":"RF-DETR Base","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Base model (29M parameters).</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRBase(RFDETR):\n    \"\"\"\n    Train an RF-DETR Base model (29M parameters).\n    \"\"\"\n    size = \"rfdetr-base\"\n    def get_model_config(self, **kwargs):\n        return RFDETRBaseConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase-attributes","title":"Attributes","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase-functions","title":"Functions","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    from roboflow import Roboflow\n    import shutil\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/train/#onnx-export) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.inference_mode():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0]\n            }\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocessors[\"bbox\"](predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        detections = sv.Detections(\n            xyxy=boxes.float().cpu().numpy(),\n            confidence=scores.float().cpu().numpy(),\n            class_id=labels.cpu().numpy(),\n        )\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/base/#rfdetr.detr.RFDETRBase.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/base/#rfdetr.detr.RFDETRBase.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/large/","title":"RF-DETR Large","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Large model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRLarge(RFDETR):\n    \"\"\"\n    Train an RF-DETR Large model.\n    \"\"\"\n    size = \"rfdetr-large\"\n    def get_model_config(self, **kwargs):\n        return RFDETRLargeConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge-attributes","title":"Attributes","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge-functions","title":"Functions","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    from roboflow import Roboflow\n    import shutil\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/train/#onnx-export) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.inference_mode():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0]\n            }\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocessors[\"bbox\"](predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        detections = sv.Detections(\n            xyxy=boxes.float().cpu().numpy(),\n            confidence=scores.float().cpu().numpy(),\n            class_id=labels.cpu().numpy(),\n        )\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/large/#rfdetr.detr.RFDETRLarge.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/medium/","title":"RF-DETR Medium","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Medium model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRMedium(RFDETR):\n    \"\"\"\n    Train an RF-DETR Medium model.\n    \"\"\"\n    size = \"rfdetr-medium\"\n    def get_model_config(self, **kwargs):\n        return RFDETRMediumConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium-attributes","title":"Attributes","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium-functions","title":"Functions","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    from roboflow import Roboflow\n    import shutil\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/train/#onnx-export) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.inference_mode():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0]\n            }\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocessors[\"bbox\"](predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        detections = sv.Detections(\n            xyxy=boxes.float().cpu().numpy(),\n            confidence=scores.float().cpu().numpy(),\n            class_id=labels.cpu().numpy(),\n        )\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/medium/#rfdetr.detr.RFDETRMedium.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/nano/","title":"RF-DETR Nano","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Nano model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRNano(RFDETR):\n    \"\"\"\n    Train an RF-DETR Nano model.\n    \"\"\"\n    size = \"rfdetr-nano\"\n    def get_model_config(self, **kwargs):\n        return RFDETRNanoConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano-attributes","title":"Attributes","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano-functions","title":"Functions","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    from roboflow import Roboflow\n    import shutil\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/train/#onnx-export) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.inference_mode():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0]\n            }\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocessors[\"bbox\"](predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        detections = sv.Detections(\n            xyxy=boxes.float().cpu().numpy(),\n            confidence=scores.float().cpu().numpy(),\n            class_id=labels.cpu().numpy(),\n        )\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/nano/#rfdetr.detr.RFDETRNano.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/rfdetr/","title":"RF-DETR","text":"<p>The base RF-DETR class implements the core methods for training RF-DETR models, running inference on the models, optimising models, and uploading trained models for deployment.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETR:\n    \"\"\"\n    The base RF-DETR class implements the core methods for training RF-DETR models,\n    running inference on the models, optimising models, and uploading trained\n    models for deployment.\n    \"\"\"\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n    size = None\n\n    def __init__(self, **kwargs):\n        self.model_config = self.get_model_config(**kwargs)\n        self.maybe_download_pretrain_weights()\n        self.model = self.get_model(self.model_config)\n        self.callbacks = defaultdict(list)\n\n        self.model.inference_model = None\n        self._is_optimized_for_inference = False\n        self._has_warned_about_not_being_optimized_for_inference = False\n        self._optimized_has_been_compiled = False\n        self._optimized_batch_size = None\n        self._optimized_resolution = None\n        self._optimized_dtype = None\n\n    def maybe_download_pretrain_weights(self):\n        \"\"\"\n        Download pre-trained weights if they are not already downloaded.\n        \"\"\"\n        download_pretrain_weights(self.model_config.pretrain_weights)\n\n    def get_model_config(self, **kwargs):\n        \"\"\"\n        Retrieve the configuration parameters used by the model.\n        \"\"\"\n        return ModelConfig(**kwargs)\n\n    def train(self, **kwargs):\n        \"\"\"\n        Train an RF-DETR model.\n        \"\"\"\n        config = self.get_train_config(**kwargs)\n        self.train_from_config(config, **kwargs)\n\n    def optimize_for_inference(self, compile=True, batch_size=1, dtype=torch.float32):\n        self.remove_optimized_model()\n\n        self.model.inference_model = deepcopy(self.model.model)\n        self.model.inference_model.eval()\n        self.model.inference_model.export()\n\n        self._optimized_resolution = self.model.resolution\n        self._is_optimized_for_inference = True\n\n        self.model.inference_model = self.model.inference_model.to(dtype=dtype)\n        self._optimized_dtype = dtype\n\n        if compile:\n            self.model.inference_model = torch.jit.trace(\n                self.model.inference_model,\n                torch.randn(\n                    batch_size, 3, self.model.resolution, self.model.resolution, \n                    device=self.model.device,\n                    dtype=dtype\n                )\n            )\n            self._optimized_has_been_compiled = True\n            self._optimized_batch_size = batch_size\n\n    def remove_optimized_model(self):\n        self.model.inference_model = None\n        self._is_optimized_for_inference = False\n        self._optimized_has_been_compiled = False\n        self._optimized_batch_size = None\n        self._optimized_resolution = None\n        self._optimized_half = False\n\n    def export(self, **kwargs):\n        \"\"\"\n        Export your model to an ONNX file.\n\n        See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/train/#onnx-export) for more information.\n        \"\"\"\n        self.model.export(**kwargs)\n\n    def train_from_config(self, config: TrainConfig, **kwargs):\n        with open(\n            os.path.join(config.dataset_dir, \"train\", \"_annotations.coco.json\"), \"r\"\n        ) as f:\n            anns = json.load(f)\n            num_classes = len(anns[\"categories\"])\n            class_names = [c[\"name\"] for c in anns[\"categories\"] if c[\"supercategory\"] != \"none\"]\n            self.model.class_names = class_names\n\n        if self.model_config.num_classes != num_classes:\n            logger.warning(\n                f\"num_classes mismatch: model has {self.model_config.num_classes} classes, but your dataset has {num_classes} classes\\n\"\n                f\"reinitializing your detection head with {num_classes} classes.\"\n            )\n            self.model.reinitialize_detection_head(num_classes)\n\n\n        train_config = config.dict()\n        model_config = self.model_config.dict()\n        model_config.pop(\"num_classes\")\n        if \"class_names\" in model_config:\n            model_config.pop(\"class_names\")\n\n        if \"class_names\" in train_config and train_config[\"class_names\"] is None:\n            train_config[\"class_names\"] = class_names\n\n        for k, v in train_config.items():\n            if k in model_config:\n                model_config.pop(k)\n            if k in kwargs:\n                kwargs.pop(k)\n\n        all_kwargs = {**model_config, **train_config, **kwargs, \"num_classes\": num_classes}\n\n        metrics_plot_sink = MetricsPlotSink(output_dir=config.output_dir)\n        self.callbacks[\"on_fit_epoch_end\"].append(metrics_plot_sink.update)\n        self.callbacks[\"on_train_end\"].append(metrics_plot_sink.save)\n\n        if config.tensorboard:\n            metrics_tensor_board_sink = MetricsTensorBoardSink(output_dir=config.output_dir)\n            self.callbacks[\"on_fit_epoch_end\"].append(metrics_tensor_board_sink.update)\n            self.callbacks[\"on_train_end\"].append(metrics_tensor_board_sink.close)\n\n        if config.wandb:\n            metrics_wandb_sink = MetricsWandBSink(\n                output_dir=config.output_dir,\n                project=config.project,\n                run=config.run,\n                config=config.model_dump()\n            )\n            self.callbacks[\"on_fit_epoch_end\"].append(metrics_wandb_sink.update)\n            self.callbacks[\"on_train_end\"].append(metrics_wandb_sink.close)\n\n        if config.early_stopping:\n            from rfdetr.util.early_stopping import EarlyStoppingCallback\n            early_stopping_callback = EarlyStoppingCallback(\n                model=self.model,\n                patience=config.early_stopping_patience,\n                min_delta=config.early_stopping_min_delta,\n                use_ema=config.early_stopping_use_ema\n            )\n            self.callbacks[\"on_fit_epoch_end\"].append(early_stopping_callback.update)\n\n        self.model.train(\n            **all_kwargs,\n            callbacks=self.callbacks,\n        )\n\n    def get_train_config(self, **kwargs):\n        \"\"\"\n        Retrieve the configuration parameters that will be used for training.\n        \"\"\"\n        return TrainConfig(**kwargs)\n\n    def get_model(self, config: ModelConfig):\n        \"\"\"\n        Retrieve a model instance based on the provided configuration.\n        \"\"\"\n        return Model(**config.dict())\n\n    # Get class_names from the model\n    @property\n    def class_names(self):\n        \"\"\"\n        Retrieve the class names supported by the loaded model.\n\n        Returns:\n            dict: A dictionary mapping class IDs to class names. The keys are integers starting from\n        \"\"\"\n        if hasattr(self.model, 'class_names') and self.model.class_names:\n            return {i+1: name for i, name in enumerate(self.model.class_names)}\n\n        return COCO_CLASSES\n\n    def predict(\n        self,\n        images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n        threshold: float = 0.5,\n        **kwargs,\n    ) -&gt; Union[sv.Detections, List[sv.Detections]]:\n        \"\"\"Performs object detection on the input images and returns bounding box\n        predictions.\n\n        This method accepts a single image or a list of images in various formats\n        (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n        RGB channel order. If a torch.Tensor is provided, it must already be normalized\n        to values in the [0, 1] range and have the shape (C, H, W).\n\n        Args:\n            images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n                A single image or a list of images to process. Images can be provided\n                as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n            threshold (float, optional):\n                The minimum confidence score needed to consider a detected bounding box valid.\n            **kwargs:\n                Additional keyword arguments.\n\n        Returns:\n            Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n                objects, each containing bounding box coordinates, confidence scores,\n                and class IDs.\n        \"\"\"\n        if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n            logger.warning(\n                \"Model is not optimized for inference. \"\n                \"Latency may be higher than expected. \"\n                \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n            )\n            self._has_warned_about_not_being_optimized_for_inference = True\n\n            self.model.model.eval()\n\n        if not isinstance(images, list):\n            images = [images]\n\n        orig_sizes = []\n        processed_images = []\n\n        for img in images:\n\n            if isinstance(img, str):\n                img = Image.open(img)\n\n            if not isinstance(img, torch.Tensor):\n                img = F.to_tensor(img)\n\n            if (img &gt; 1).any():\n                raise ValueError(\n                    \"Image has pixel values above 1. Please ensure the image is \"\n                    \"normalized (scaled to [0, 1]).\"\n                )\n            if img.shape[0] != 3:\n                raise ValueError(\n                    f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                    f\"{img.shape[0]} channels.\"\n                )\n            img_tensor = img\n\n            h, w = img_tensor.shape[1:]\n            orig_sizes.append((h, w))\n\n            img_tensor = img_tensor.to(self.model.device)\n            img_tensor = F.normalize(img_tensor, self.means, self.stds)\n            img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n            processed_images.append(img_tensor)\n\n        batch_tensor = torch.stack(processed_images)\n\n        if self._is_optimized_for_inference:\n            if self._optimized_resolution != batch_tensor.shape[2]:\n                # this could happen if someone manually changes self.model.resolution after optimizing the model\n                raise ValueError(f\"Resolution mismatch. \"\n                                 f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                                 f\"but got {batch_tensor.shape[2]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n            if self._optimized_has_been_compiled:\n                if self._optimized_batch_size != batch_tensor.shape[0]:\n                    raise ValueError(f\"Batch size mismatch. \"\n                                     f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                     f\"but got {batch_tensor.shape[0]}. \"\n                                     \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                     \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                     \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n        with torch.inference_mode():\n            if self._is_optimized_for_inference:\n                predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n            else:\n                predictions = self.model.model(batch_tensor)\n            if isinstance(predictions, tuple):\n                predictions = {\n                    \"pred_logits\": predictions[1],\n                    \"pred_boxes\": predictions[0]\n                }\n            target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n            results = self.model.postprocessors[\"bbox\"](predictions, target_sizes=target_sizes)\n\n        detections_list = []\n        for result in results:\n            scores = result[\"scores\"]\n            labels = result[\"labels\"]\n            boxes = result[\"boxes\"]\n\n            keep = scores &gt; threshold\n            scores = scores[keep]\n            labels = labels[keep]\n            boxes = boxes[keep]\n\n            detections = sv.Detections(\n                xyxy=boxes.float().cpu().numpy(),\n                confidence=scores.float().cpu().numpy(),\n                class_id=labels.cpu().numpy(),\n            )\n            detections_list.append(detections)\n\n        return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n\n    def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n        \"\"\"\n        Deploy the trained RF-DETR model to Roboflow.\n\n        Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n        You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n        Args:\n            workspace (str): The name of the Roboflow workspace to deploy to.\n            project_ids (List[str]): A list of project IDs to which the model will be deployed\n            api_key (str, optional): Your Roboflow API key. If not provided,\n                it will be read from the environment variable `ROBOFLOW_API_KEY`.\n            size (str, optional): The size of the model to deploy. If not provided,\n                it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n            model_name (str, optional): The name you want to give the uploaded model.\n            If not provided, it will default to \"&lt;size&gt;-uploaded\".\n        Raises:\n            ValueError: If the `api_key` is not provided and not found in the environment\n                variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n        \"\"\"\n        from roboflow import Roboflow\n        import shutil\n        if api_key is None:\n            api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n            if api_key is None:\n                raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n        rf = Roboflow(api_key=api_key)\n        workspace = rf.workspace(workspace)\n\n        if self.size is None and size is None:\n            raise ValueError(\"Must set size for custom architectures\")\n\n        size = self.size or size\n        tmp_out_dir = \".roboflow_temp_upload\"\n        os.makedirs(tmp_out_dir, exist_ok=True)\n        outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n        torch.save(\n            {\n                \"model\": self.model.model.state_dict(),\n                \"args\": self.model.args\n            }, outpath\n        )\n        project = workspace.project(project_id)\n        version = project.version(version)\n        version.deploy(\n            model_type=size,\n            model_path=tmp_out_dir,\n            filename=\"weights.pt\"\n        )\n        shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR-attributes","title":"Attributes","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR-functions","title":"Functions","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    from roboflow import Roboflow\n    import shutil\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/train/#onnx-export) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.get_model_config","title":"<code>get_model_config(**kwargs)</code>","text":"<p>Retrieve the configuration parameters used by the model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model_config(self, **kwargs):\n    \"\"\"\n    Retrieve the configuration parameters used by the model.\n    \"\"\"\n    return ModelConfig(**kwargs)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.get_train_config","title":"<code>get_train_config(**kwargs)</code>","text":"<p>Retrieve the configuration parameters that will be used for training.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_train_config(self, **kwargs):\n    \"\"\"\n    Retrieve the configuration parameters that will be used for training.\n    \"\"\"\n    return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.inference_mode():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0]\n            }\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocessors[\"bbox\"](predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        detections = sv.Detections(\n            xyxy=boxes.float().cpu().numpy(),\n            confidence=scores.float().cpu().numpy(),\n            class_id=labels.cpu().numpy(),\n        )\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/rfdetr/#rfdetr.detr.RFDETR.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"},{"location":"reference/small/","title":"RF-DETR Small","text":"<p>               Bases: <code>RFDETR</code></p> <p>Train an RF-DETR Small model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>class RFDETRSmall(RFDETR):\n    \"\"\"\n    Train an RF-DETR Small model.\n    \"\"\"\n    size = \"rfdetr-small\"\n    def get_model_config(self, **kwargs):\n        return RFDETRSmallConfig(**kwargs)\n\n    def get_train_config(self, **kwargs):\n        return TrainConfig(**kwargs)\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall-attributes","title":"Attributes","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Retrieve the class names supported by the loaded model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping class IDs to class names. The keys are integers starting from</p>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall-functions","title":"Functions","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow","title":"<code>deploy_to_roboflow(workspace, project_id, version, api_key=None, size=None)</code>","text":"<p>Deploy the trained RF-DETR model to Roboflow.</p> <p>Deploying with Roboflow will create a Serverless API to which you can make requests.</p> <p>You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the Roboflow workspace to deploy to.</p> required <code>List[str]</code> <p>A list of project IDs to which the model will be deployed</p> required <code>str</code> <p>Your Roboflow API key. If not provided, it will be read from the environment variable <code>ROBOFLOW_API_KEY</code>.</p> <code>None</code> <code>str</code> <p>The size of the model to deploy. If not provided, it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).</p> <code>None</code> <code>str</code> <p>The name you want to give the uploaded model.</p> required <p>Raises:     ValueError: If the <code>api_key</code> is not provided and not found in the environment         variable <code>ROBOFLOW_API_KEY</code>, or if the <code>size</code> is not set for custom architectures.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def deploy_to_roboflow(self, workspace: str, project_id: str, version: str, api_key: str = None, size: str = None):\n    \"\"\"\n    Deploy the trained RF-DETR model to Roboflow.\n\n    Deploying with Roboflow will create a Serverless API to which you can make requests.\n\n    You can also download weights into a Roboflow Inference deployment for use in Roboflow Workflows and on-device deployment.\n\n    Args:\n        workspace (str): The name of the Roboflow workspace to deploy to.\n        project_ids (List[str]): A list of project IDs to which the model will be deployed\n        api_key (str, optional): Your Roboflow API key. If not provided,\n            it will be read from the environment variable `ROBOFLOW_API_KEY`.\n        size (str, optional): The size of the model to deploy. If not provided,\n            it will default to the size of the model being trained (e.g., \"rfdetr-base\", \"rfdetr-large\", etc.).\n        model_name (str, optional): The name you want to give the uploaded model.\n        If not provided, it will default to \"&lt;size&gt;-uploaded\".\n    Raises:\n        ValueError: If the `api_key` is not provided and not found in the environment\n            variable `ROBOFLOW_API_KEY`, or if the `size` is not set for custom architectures.\n    \"\"\"\n    from roboflow import Roboflow\n    import shutil\n    if api_key is None:\n        api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Set api_key=&lt;KEY&gt; in deploy_to_roboflow or export ROBOFLOW_API_KEY=&lt;KEY&gt;\")\n\n\n    rf = Roboflow(api_key=api_key)\n    workspace = rf.workspace(workspace)\n\n    if self.size is None and size is None:\n        raise ValueError(\"Must set size for custom architectures\")\n\n    size = self.size or size\n    tmp_out_dir = \".roboflow_temp_upload\"\n    os.makedirs(tmp_out_dir, exist_ok=True)\n    outpath = os.path.join(tmp_out_dir, \"weights.pt\")\n    torch.save(\n        {\n            \"model\": self.model.model.state_dict(),\n            \"args\": self.model.args\n        }, outpath\n    )\n    project = workspace.project(project_id)\n    version = project.version(version)\n    version.deploy(\n        model_type=size,\n        model_path=tmp_out_dir,\n        filename=\"weights.pt\"\n    )\n    shutil.rmtree(tmp_out_dir)\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(workspace)","title":"<code>workspace</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(project_ids)","title":"<code>project_ids</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(api_key)","title":"<code>api_key</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(size)","title":"<code>size</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.deploy_to_roboflow(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.export","title":"<code>export(**kwargs)</code>","text":"<p>Export your model to an ONNX file.</p> <p>See the ONNX export documentation for more information.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def export(self, **kwargs):\n    \"\"\"\n    Export your model to an ONNX file.\n\n    See [the ONNX export documentation](https://rfdetr.roboflow.com/learn/train/#onnx-export) for more information.\n    \"\"\"\n    self.model.export(**kwargs)\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.get_model","title":"<code>get_model(config)</code>","text":"<p>Retrieve a model instance based on the provided configuration.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def get_model(self, config: ModelConfig):\n    \"\"\"\n    Retrieve a model instance based on the provided configuration.\n    \"\"\"\n    return Model(**config.dict())\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.maybe_download_pretrain_weights","title":"<code>maybe_download_pretrain_weights()</code>","text":"<p>Download pre-trained weights if they are not already downloaded.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def maybe_download_pretrain_weights(self):\n    \"\"\"\n    Download pre-trained weights if they are not already downloaded.\n    \"\"\"\n    download_pretrain_weights(self.model_config.pretrain_weights)\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.predict","title":"<code>predict(images, threshold=0.5, **kwargs)</code>","text":"<p>Performs object detection on the input images and returns bounding box predictions.</p> <p>This method accepts a single image or a list of images in various formats (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in RGB channel order. If a torch.Tensor is provided, it must already be normalized to values in the [0, 1] range and have the shape (C, H, W).</p> <p>Parameters:</p> Name Type Description Default <code>Union[str, Image, ndarray, Tensor, List[Union[str, ndarray, Image, Tensor]]]</code> <p>A single image or a list of images to process. Images can be provided as file paths, PIL Images, NumPy arrays, or torch.Tensors.</p> required <code>float</code> <p>The minimum confidence score needed to consider a detected bounding box valid.</p> <code>0.5</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Detections, List[Detections]]</code> <p>Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections objects, each containing bounding box coordinates, confidence scores, and class IDs.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def predict(\n    self,\n    images: Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; Union[sv.Detections, List[sv.Detections]]:\n    \"\"\"Performs object detection on the input images and returns bounding box\n    predictions.\n\n    This method accepts a single image or a list of images in various formats\n    (file path, PIL Image, NumPy array, or torch.Tensor). The images should be in\n    RGB channel order. If a torch.Tensor is provided, it must already be normalized\n    to values in the [0, 1] range and have the shape (C, H, W).\n\n    Args:\n        images (Union[str, Image.Image, np.ndarray, torch.Tensor, List[Union[str, np.ndarray, Image.Image, torch.Tensor]]]):\n            A single image or a list of images to process. Images can be provided\n            as file paths, PIL Images, NumPy arrays, or torch.Tensors.\n        threshold (float, optional):\n            The minimum confidence score needed to consider a detected bounding box valid.\n        **kwargs:\n            Additional keyword arguments.\n\n    Returns:\n        Union[sv.Detections, List[sv.Detections]]: A single or multiple Detections\n            objects, each containing bounding box coordinates, confidence scores,\n            and class IDs.\n    \"\"\"\n    if not self._is_optimized_for_inference and not self._has_warned_about_not_being_optimized_for_inference:\n        logger.warning(\n            \"Model is not optimized for inference. \"\n            \"Latency may be higher than expected. \"\n            \"You can optimize the model for inference by calling model.optimize_for_inference().\"\n        )\n        self._has_warned_about_not_being_optimized_for_inference = True\n\n        self.model.model.eval()\n\n    if not isinstance(images, list):\n        images = [images]\n\n    orig_sizes = []\n    processed_images = []\n\n    for img in images:\n\n        if isinstance(img, str):\n            img = Image.open(img)\n\n        if not isinstance(img, torch.Tensor):\n            img = F.to_tensor(img)\n\n        if (img &gt; 1).any():\n            raise ValueError(\n                \"Image has pixel values above 1. Please ensure the image is \"\n                \"normalized (scaled to [0, 1]).\"\n            )\n        if img.shape[0] != 3:\n            raise ValueError(\n                f\"Invalid image shape. Expected 3 channels (RGB), but got \"\n                f\"{img.shape[0]} channels.\"\n            )\n        img_tensor = img\n\n        h, w = img_tensor.shape[1:]\n        orig_sizes.append((h, w))\n\n        img_tensor = img_tensor.to(self.model.device)\n        img_tensor = F.normalize(img_tensor, self.means, self.stds)\n        img_tensor = F.resize(img_tensor, (self.model.resolution, self.model.resolution))\n\n        processed_images.append(img_tensor)\n\n    batch_tensor = torch.stack(processed_images)\n\n    if self._is_optimized_for_inference:\n        if self._optimized_resolution != batch_tensor.shape[2]:\n            # this could happen if someone manually changes self.model.resolution after optimizing the model\n            raise ValueError(f\"Resolution mismatch. \"\n                             f\"Model was optimized for resolution {self._optimized_resolution}, \"\n                             f\"but got {batch_tensor.shape[2]}. \"\n                             \"You can explicitly remove the optimized model by calling model.remove_optimized_model().\")\n        if self._optimized_has_been_compiled:\n            if self._optimized_batch_size != batch_tensor.shape[0]:\n                raise ValueError(f\"Batch size mismatch. \"\n                                 f\"Optimized model was compiled for batch size {self._optimized_batch_size}, \"\n                                 f\"but got {batch_tensor.shape[0]}. \"\n                                 \"You can explicitly remove the optimized model by calling model.remove_optimized_model(). \"\n                                 \"Alternatively, you can recompile the optimized model for a different batch size \"\n                                 \"by calling model.optimize_for_inference(batch_size=&lt;new_batch_size&gt;).\")\n\n    with torch.inference_mode():\n        if self._is_optimized_for_inference:\n            predictions = self.model.inference_model(batch_tensor.to(dtype=self._optimized_dtype))\n        else:\n            predictions = self.model.model(batch_tensor)\n        if isinstance(predictions, tuple):\n            predictions = {\n                \"pred_logits\": predictions[1],\n                \"pred_boxes\": predictions[0]\n            }\n        target_sizes = torch.tensor(orig_sizes, device=self.model.device)\n        results = self.model.postprocessors[\"bbox\"](predictions, target_sizes=target_sizes)\n\n    detections_list = []\n    for result in results:\n        scores = result[\"scores\"]\n        labels = result[\"labels\"]\n        boxes = result[\"boxes\"]\n\n        keep = scores &gt; threshold\n        scores = scores[keep]\n        labels = labels[keep]\n        boxes = boxes[keep]\n\n        detections = sv.Detections(\n            xyxy=boxes.float().cpu().numpy(),\n            confidence=scores.float().cpu().numpy(),\n            class_id=labels.cpu().numpy(),\n        )\n        detections_list.append(detections)\n\n    return detections_list if len(detections_list) &gt; 1 else detections_list[0]\n</code></pre>"},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.predict(images)","title":"<code>images</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.predict(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.predict(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/small/#rfdetr.detr.RFDETRSmall.train","title":"<code>train(**kwargs)</code>","text":"<p>Train an RF-DETR model.</p> Source code in <code>rfdetr/detr.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    Train an RF-DETR model.\n    \"\"\"\n    config = self.get_train_config(**kwargs)\n    self.train_from_config(config, **kwargs)\n</code></pre>"}]}